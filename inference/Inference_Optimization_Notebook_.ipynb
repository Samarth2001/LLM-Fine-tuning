{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kifHB01AETa4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft accelerate bitsandbytes torch -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fsfx3dgUslA"
      },
      "outputs": [],
      "source": [
        "!pip install optimum -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "e79adeb1db884c08b50ce111fd4197e9",
            "602560880d6a466db9d768b8ef9bfee4",
            "d944fd19f3344cc6a723e41c46a50049",
            "0f86832699274aa9a92b7da03d7c93fa",
            "8810df0de6314d0d9e87c5b82f2b6f49",
            "f0751f78ea3348848b09498f27a5f444",
            "3b99e2a0774046c2aea5eb0d1aa034cc",
            "87bb560d63cb4e918353a34bbf5af4f0",
            "03f800e6a2e54f2a871cc143fd26b707",
            "8f696168f4ce489cb40cc248312ac3f3",
            "8862ba581520401d8b63c69dd3ce6ce1"
          ]
        },
        "id": "gVbXfhxcEytk",
        "outputId": "9f1178ef-35bf-48b0-d04a-0de19e2484fe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import json\n",
        "import gc\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWrExXunFkmR"
      },
      "source": [
        "# **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAZyKvLiFYSm",
        "outputId": "c85db5d3-6a11-4d32-9dff-be664e11afec"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive first\n",
        "\n",
        "# Path to your fine-tuned model in Drive (should be the directory)\n",
        "MODEL_PATH = \"/content/drive/MyDrive/my_finetuned_codegen_model\"\n",
        "# Alternative paths (update to match where you saved):\n",
        "# MODEL_PATH = \"/content/drive/MyDrive/models/codegen_finetuned_lora\"\n",
        "# MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/codegen_finetuned_lora\"\n",
        "\n",
        "# Test configuration\n",
        "TEST_PROMPTS = [\n",
        "    \"# Write a function to calculate factorial\\n\",\n",
        "    \"# Create a class for a binary tree\\n\",\n",
        "    \"# Implement bubble sort\\n\",\n",
        "    \"# Check if a string is palindrome\\n\"\n",
        "]\n",
        "\n",
        "# Optimization settings\n",
        "RUN_8BIT_TEST = True\n",
        "RUN_COMPILE_TEST = True if hasattr(torch, 'compile') else False\n",
        "RUN_BATCH_TEST = True\n",
        "\n",
        "print(f\"Model path: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X60A6VWEFYPK",
        "outputId": "0b29a6a4-6b78-4b0c-9a31-1a9064bb4034"
      },
      "outputs": [],
      "source": [
        "print(\"Loading fine-tuned model from Google Drive...\")\n",
        "\n",
        "# Set offline mode to avoid HuggingFace connection\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
        "\n",
        "try:\n",
        "    # Attempt to load the fine-tuned model directly\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    print(\"Loaded fine-tuned model directly from Google Drive\")\n",
        "    is_lora = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model directly: {e}\")\n",
        "    print(\"Attempting to load as LoRA model by first loading the base model...\")\n",
        "    is_lora = True\n",
        "\n",
        "    try:\n",
        "        # Load PEFT config manually from the local path\n",
        "        import json\n",
        "        config_path = os.path.join(MODEL_PATH, \"adapter_config.json\")\n",
        "        with open(config_path, \"r\") as f:\n",
        "            config_data = json.load(f)\n",
        "\n",
        "        # The base model name\n",
        "        base_model_name = config_data.get(\"base_model_name_or_path\")\n",
        "        if base_model_name is None:\n",
        "            raise ValueError(\"Could not find 'base_model_name_or_path' in adapter_config.json\")\n",
        "        print(f\"Base model: {base_model_name}\")\n",
        "\n",
        "\n",
        "        # Load base model from cache or local files\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        # Load LoRA weights from Drive\n",
        "        model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
        "        print(\"Loaded as LoRA model from Google Drive\")\n",
        "        is_lora = True\n",
        "\n",
        "    except Exception as e_lora:\n",
        "        print(f\"Error loading as LoRA model: {e_lora}\")\n",
        "        print(\"Model loading failed using both direct and LoRA methods.\")\n",
        "        is_lora = False\n",
        "\n",
        "\n",
        "# Load tokenizer from the local path after copying\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if 'model' in locals() and model is not None:\n",
        "    print(f\"Model loaded successfully! Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
        "else:\n",
        "    print(\"Model loading failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpSzbPKpPbeg"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(model, tokenizer, prompts, num_runs=3, max_tokens=100):\n",
        "    \"\"\"Comprehensive benchmark function\"\"\"\n",
        "    times = []\n",
        "    tokens_generated = []\n",
        "\n",
        "    # Warmup\n",
        "    warmup_prompt = prompts[0] if isinstance(prompts, list) else prompts\n",
        "    inputs = tokenizer(warmup_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    _ = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "    # Actual benchmark\n",
        "    for run in range(num_runs):\n",
        "        if isinstance(prompts, list):\n",
        "            # Batch processing\n",
        "            for prompt in prompts:\n",
        "                start = time.time()\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "                torch.cuda.synchronize()\n",
        "                elapsed = time.time() - start\n",
        "                times.append(elapsed)\n",
        "                tokens_generated.append(len(outputs[0]) - len(inputs['input_ids'][0]))\n",
        "        else:\n",
        "            # Single prompt\n",
        "            start = time.time()\n",
        "            inputs = tokenizer(prompts, return_tensors=\"pt\").to(\"cuda\")\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed = time.time() - start\n",
        "            times.append(elapsed)\n",
        "            tokens_generated.append(len(outputs[0]) - len(inputs['input_ids'][0]))\n",
        "\n",
        "    avg_time = np.mean(times)\n",
        "    avg_tokens = np.mean(tokens_generated)\n",
        "    tokens_per_sec = avg_tokens / avg_time\n",
        "\n",
        "    return {\n",
        "        \"avg_time\": avg_time,\n",
        "        \"tokens_per_sec\": tokens_per_sec,\n",
        "        \"total_tokens\": sum(tokens_generated),\n",
        "        \"runs\": num_runs,\n",
        "        \"memory_gb\": torch.cuda.memory_allocated()/1024**3\n",
        "    }\n",
        "\n",
        "def print_benchmark_results(name, results, baseline=None):\n",
        "    \"\"\"Pretty print benchmark results\"\"\"\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"  Average time: {results['avg_time']:.2f}s\")\n",
        "    print(f\"  Tokens/sec: {results['tokens_per_sec']:.1f}\")\n",
        "    print(f\"  Memory: {results['memory_gb']:.2f}GB\")\n",
        "\n",
        "    if baseline:\n",
        "        speedup = results['tokens_per_sec'] / baseline['tokens_per_sec']\n",
        "        memory_reduction = (baseline['memory_gb'] - results['memory_gb']) / baseline['memory_gb'] * 100\n",
        "        print(f\"  Speedup: {speedup:.2f}x\")\n",
        "        if memory_reduction > 0:\n",
        "            print(f\"  Memory reduction: {memory_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VVkSlPOVdDg"
      },
      "outputs": [],
      "source": [
        "TEST_PROMPTS = [\n",
        "    \"# Write a function to calculate factorial\\n\",\n",
        "    \"# Create a class for a binary tree\\n\",\n",
        "    \"# Implement bubble sort\\n\",\n",
        "    \"# Check if a string is palindrome\\n\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFIaMGjTWUtP"
      },
      "outputs": [],
      "source": [
        "RUN_8BIT_TEST = True\n",
        "RUN_COMPILE_TEST = True if hasattr(torch, 'compile') else False\n",
        "RUN_BATCH_TEST = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8c4ShGNPbc5",
        "outputId": "b726b753-ed16-430e-92fc-cb487ea44cc1"
      },
      "outputs": [],
      "source": [
        "print(\"BASELINE PERFORMANCE TEST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test with single prompt\n",
        "single_prompt = TEST_PROMPTS[0]\n",
        "print(f\"Testing with: {single_prompt.strip()}\")\n",
        "\n",
        "baseline_results = benchmark_inference(model, tokenizer, single_prompt)\n",
        "print_benchmark_results(\"Baseline Performance\", baseline_results)\n",
        "\n",
        "# Save baseline for comparison\n",
        "all_results = {\"baseline\": baseline_results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flDo8m3wPbba",
        "outputId": "a4ac5182-43f6-4276-a88a-128cf72ea4d2"
      },
      "outputs": [],
      "source": [
        "if is_lora: # Only attempt merge if loaded as LoRA\n",
        "    print(\"\\nOPTIMIZATION 1: Merge LoRA Weights\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Merge LoRA weights\n",
        "    print(\"Merging LoRA weights...\")\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    # Benchmark merged model\n",
        "    merge_results = benchmark_inference(merged_model, tokenizer, single_prompt)\n",
        "    print_benchmark_results(\"Merged Model\", merge_results, baseline_results)\n",
        "    all_results[\"merged\"] = merge_results\n",
        "\n",
        "    # Use merged model going forward\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    model = merged_model\n",
        "    is_lora = False\n",
        "else:\n",
        "    print(\"\\nModel already merged or not a LoRA model, skipping LoRA merge test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "l_qAutRxPbY0",
        "outputId": "0dd158bc-5137-4d0c-af31-281293ffde66"
      },
      "outputs": [],
      "source": [
        "if RUN_8BIT_TEST:\n",
        "    print(\"\\nOPTIMIZATION 2: 8-bit Inference\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Save current model first\n",
        "    save_path = \"./temp_model\"\n",
        "    # Revert from BetterTransformer before saving if it was applied\n",
        "    if hasattr(model, 'reverse_bettertransformer'):\n",
        "        model = model.reverse_bettertransformer()\n",
        "        print(\"Reverted model from BetterTransformer state.\")\n",
        "\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Reload in 8-bit\n",
        "    print(\"Reloading model in 8-bit...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        bnb_8bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "        save_path,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Benchmark 8-bit model\n",
        "    bit8_results = benchmark_inference(model_8bit, tokenizer, single_prompt)\n",
        "    print_benchmark_results(\"8-bit Model\", bit8_results, baseline_results)\n",
        "    all_results[\"8bit\"] = bit8_results\n",
        "\n",
        "    # Continue with 8-bit model\n",
        "    model = model_8bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nNlTAnCYFfx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9GeV_YJQ884",
        "outputId": "469d94d7-f75d-4c5a-d56b-3627a229981b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nOPTIMIZATION 3: Better Transformers\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    # Try to convert to BetterTransformer\n",
        "    model_bt = model.to_bettertransformer()\n",
        "\n",
        "    # Benchmark\n",
        "    bt_results = benchmark_inference(model_bt, tokenizer, single_prompt)\n",
        "    print_benchmark_results(\"BetterTransformer\", bt_results, baseline_results)\n",
        "    all_results[\"better_transformer\"] = bt_results\n",
        "\n",
        "    model = model_bt\n",
        "    print(\"Using BetterTransformer for remaining tests\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"BetterTransformer not supported: {str(e)[:100]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ9NJossRB7I"
      },
      "outputs": [],
      "source": [
        "if RUN_COMPILE_TEST and hasattr(torch, 'compile'):\n",
        "    print(\"\\nOPTIMIZATION 4: Torch Compile\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"Compiling model (this takes 2-3 minutes)...\")\n",
        "    model_compiled = torch.compile(model, mode=\"reduce-overhead\")\n",
        "\n",
        "    # Warmup compilation\n",
        "    print(\"Warming up...\")\n",
        "    _ = model_compiled.generate(\n",
        "        tokenizer(\"# test\", return_tensors=\"pt\").to(\"cuda\").input_ids,\n",
        "        max_new_tokens=10\n",
        "    )\n",
        "\n",
        "    # Benchmark\n",
        "    compile_results = benchmark_inference(model_compiled, tokenizer, single_prompt)\n",
        "    print_benchmark_results(\"Compiled Model\", compile_results, baseline_results)\n",
        "    all_results[\"compiled\"] = compile_results\n",
        "\n",
        "    model = model_compiled\n",
        "else:\n",
        "    print(\"\\nSkipping Torch Compile (not available or disabled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq0uvLXURFH1"
      },
      "outputs": [],
      "source": [
        "print(\"\\nOPTIMIZATION 5: Optimized Generation Config\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create optimized generation config\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    use_cache=True,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    num_beams=1,  # Greedy is fastest\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# Custom benchmark with generation config\n",
        "def benchmark_with_config(model, tokenizer, prompt, config, num_runs=3):\n",
        "    times = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        start = time.time()\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(**inputs, generation_config=config)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        times.append(time.time() - start)\n",
        "\n",
        "    return {\n",
        "        \"avg_time\": np.mean(times),\n",
        "        \"tokens_per_sec\": 100 / np.mean(times),\n",
        "        \"memory_gb\": torch.cuda.memory_allocated()/1024**3\n",
        "    }\n",
        "\n",
        "config_results = benchmark_with_config(model, tokenizer, single_prompt, gen_config)\n",
        "print_benchmark_results(\"Optimized Config\", config_results, baseline_results)\n",
        "all_results[\"optimized_config\"] = config_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm1MW71XRHUH"
      },
      "outputs": [],
      "source": [
        "if RUN_BATCH_TEST:\n",
        "    print(\"\\nOPTIMIZATION 6: Batch Processing\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    batch_sizes = [1, 2, 4]\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        if batch_size > len(TEST_PROMPTS):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTesting batch size: {batch_size}\")\n",
        "\n",
        "        # Create batch\n",
        "        batch_prompts = TEST_PROMPTS[:batch_size]\n",
        "\n",
        "        # Time batch processing\n",
        "        start = time.time()\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        batch_time = time.time() - start\n",
        "\n",
        "        tokens_per_prompt = 100\n",
        "        total_tokens = tokens_per_prompt * batch_size\n",
        "        tokens_per_sec = total_tokens / batch_time\n",
        "\n",
        "        print(f\"  Total time: {batch_time:.2f}s\")\n",
        "        print(f\"  Time per prompt: {batch_time/batch_size:.2f}s\")\n",
        "        print(f\"  Tokens/sec (total): {tokens_per_sec:.1f}\")\n",
        "        print(f\"  Speedup: {tokens_per_sec/(baseline_results['tokens_per_sec']*batch_size):.2f}x\")\n",
        "\n",
        "        all_results[f\"batch_{batch_size}\"] = {\n",
        "            \"tokens_per_sec\": tokens_per_sec,\n",
        "            \"time_per_prompt\": batch_time/batch_size\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urMpkVUdRJSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\nOPTIMIZATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comparison table\n",
        "print(f\"\\n{'Method':<25} {'Tokens/sec':<12} {'Speedup':<10} {'Memory (GB)':<12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "baseline_tps = baseline_results['tokens_per_sec']\n",
        "\n",
        "for name, results in all_results.items():\n",
        "    if isinstance(results, dict) and 'tokens_per_sec' in results:\n",
        "        tps = results['tokens_per_sec']\n",
        "        speedup = tps / baseline_tps\n",
        "        memory = results.get('memory_gb', 'N/A')\n",
        "\n",
        "        if isinstance(memory, float):\n",
        "            memory_str = f\"{memory:.2f}\"\n",
        "        else:\n",
        "            memory_str = str(memory)\n",
        "\n",
        "        print(f\"{name:<25} {tps:<12.1f} {speedup:<10.2f}x {memory_str:<12}\")\n",
        "\n",
        "# Find best configuration\n",
        "best_single = max(\n",
        "    [(k, v) for k, v in all_results.items() if not k.startswith('batch')],\n",
        "    key=lambda x: x[1].get('tokens_per_sec', 0)\n",
        ")\n",
        "\n",
        "print(f\"\\nBest single-prompt optimization: {best_single[0]}\")\n",
        "print(f\"   Speed: {best_single[1]['tokens_per_sec']:.1f} tokens/sec\")\n",
        "print(f\"   Speedup: {best_single[1]['tokens_per_sec']/baseline_tps:.2f}x\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f800e6a2e54f2a871cc143fd26b707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f86832699274aa9a92b7da03d7c93fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f696168f4ce489cb40cc248312ac3f3",
            "placeholder": "​",
            "style": "IPY_MODEL_8862ba581520401d8b63c69dd3ce6ce1",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "3b99e2a0774046c2aea5eb0d1aa034cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "602560880d6a466db9d768b8ef9bfee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0751f78ea3348848b09498f27a5f444",
            "placeholder": "​",
            "style": "IPY_MODEL_3b99e2a0774046c2aea5eb0d1aa034cc",
            "value": ""
          }
        },
        "87bb560d63cb4e918353a34bbf5af4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8810df0de6314d0d9e87c5b82f2b6f49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8862ba581520401d8b63c69dd3ce6ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f696168f4ce489cb40cc248312ac3f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d944fd19f3344cc6a723e41c46a50049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87bb560d63cb4e918353a34bbf5af4f0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03f800e6a2e54f2a871cc143fd26b707",
            "value": 0
          }
        },
        "e79adeb1db884c08b50ce111fd4197e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_602560880d6a466db9d768b8ef9bfee4",
              "IPY_MODEL_d944fd19f3344cc6a723e41c46a50049",
              "IPY_MODEL_0f86832699274aa9a92b7da03d7c93fa"
            ],
            "layout": "IPY_MODEL_8810df0de6314d0d9e87c5b82f2b6f49"
          }
        },
        "f0751f78ea3348848b09498f27a5f444": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
