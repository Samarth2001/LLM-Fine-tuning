{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers accelerate peft bitsandbytes datasets trl scipy triton\n\n!pip install --upgrade transformers==4.41.2 -q\n!pip install --upgrade peft==0.11.1 -q\n!pip install --upgrade accelerate==0.30.1 -q\n!pip install bitsandbytes -q\n!pip install --upgrade datasets==2.19.1 -q\n!pip install --upgrade trl==0.8.6 -q\n!pip install --upgrade scipy -q\n!pip install --upgrade triton -q\n!pip install -q torch\n!pip install -q sentencepiece\n!pip install -q einops\n\n# Install bitsandbytes from alternative source for better compatibility with colab\n!pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n# Restart runtime to apply changes\nimport os\nos.kill(os.getpid(), 9)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:28:18.065316Z","iopub.execute_input":"2025-07-25T13:28:18.065943Z","execution_failed":"2025-07-25T13:30:46.249Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.52.4\nUninstalling transformers-4.52.4:\n  Successfully uninstalled transformers-4.52.4\nFound existing installation: accelerate 1.8.1\nUninstalling accelerate-1.8.1:\n  Successfully uninstalled accelerate-1.8.1\nFound existing installation: peft 0.15.2\nUninstalling peft-0.15.2:\n  Successfully uninstalled peft-0.15.2\n\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: datasets 3.6.0\nUninstalling datasets-3.6.0:\n  Successfully uninstalled datasets-3.6.0\n\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: scipy 1.15.3\nUninstalling scipy-1.15.3:\n  Successfully uninstalled scipy-1.15.3\nFound existing installation: triton 3.2.0\nUninstalling triton-3.2.0:\n  Successfully uninstalled triton-3.2.0\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires datasets, which is not installed.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\nsentence-transformers 4.1.0 requires scipy, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\nfastai 2.7.19 requires scipy, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\ncesium 0.12.4 requires scipy>=0.16.0, which is not installed.\nsentence-transformers 4.1.0 requires scipy, which is not installed.\nfastai 2.7.19 requires scipy, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mLooking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:31:58.475953Z","iopub.execute_input":"2025-07-25T13:31:58.476532Z","iopub.status.idle":"2025-07-25T13:31:58.479931Z","shell.execute_reply.started":"2025-07-25T13:31:58.476505Z","shell.execute_reply":"2025-07-25T13:31:58.479293Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    get_linear_schedule_with_warmup\n)\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\nimport json\nimport random\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Tuple, Optional, Any\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport gc\n\n# Configuration\n@dataclass\nclass ReasonDPOConfig:\n    # Model settings\n    model_name: str = \"HuggingfaceTB/SmolLM2-360M-Instruct\"\n    ref_model_name: str = None\n\n    # DPO settings\n    beta: float = 0.1\n    learning_rate: float = 5e-7\n    batch_size: int = 32\n    gradient_accumulation_steps: int = 2\n    num_epochs: int = 1  # Reduced for faster training\n    max_length: int = 1024\n    max_prompt_length: int = 512\n\n    # LoRA settings\n    lora_r: int = 16  # Reduced for fewer parameters\n    lora_alpha: int = 64\n    lora_dropout: float = 0.05\n\n    # Data settings\n    num_train_preferences: int = 200  # Drastically reduced for speed\n    num_eval_preferences: int = 40    # Drastically reduced for speed\n    responses_per_prompt: int = 4     # Reduced for faster data generation\n\n    # Generation settings\n    generation_temperature: float = 0.8\n    generation_max_tokens: int = 300\n\n    # Paths\n    output_dir: str = \"./reason_dpo_output\"\n    cache_dir: str = \"./cache\"\n\nconfig = ReasonDPOConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.744399Z","iopub.execute_input":"2025-07-25T13:41:21.744710Z","iopub.status.idle":"2025-07-25T13:41:21.753798Z","shell.execute_reply.started":"2025-07-25T13:41:21.744692Z","shell.execute_reply":"2025-07-25T13:41:21.753034Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class AdvancedReasoningTasks:\n    \"\"\"Generate diverse reasoning tasks for DPO training\"\"\"\n    \n    def __init__(self, seed=42):\n        random.seed(seed)\n        np.random.seed(seed)\n        self.task_types = [\n            self.generate_logical_deduction,\n            self.generate_mathematical_reasoning,\n            self.generate_pattern_completion,\n            self.generate_causal_reasoning,\n            self.generate_analogical_reasoning,\n            self.generate_constraint_solving,\n            self.generate_counterfactual_reasoning\n        ]\n    \n    def generate_logical_deduction(self):\n        \"\"\"Multi-step logical deduction\"\"\"\n        premises = [\n            (\"All {A} are {B}\", \"All {B} are {C}\", \"All {A} are {C}\"),\n            (\"No {A} are {B}\", \"All {C} are {B}\", \"No {A} are {C}\"),\n            (\"Some {A} are {B}\", \"All {B} are {C}\", \"Some {A} are {C}\"),\n        ]\n        \n        entities = [\n            (\"cats\", \"mammals\", \"animals\"),\n            (\"birds\", \"flying creatures\", \"egg-layers\"),\n            (\"roses\", \"flowers\", \"plants\"),\n            (\"diamonds\", \"gems\", \"minerals\")\n        ]\n        \n        premise_template = random.choice(premises)\n        entity_set = random.choice(entities)\n        \n        premises_text = []\n        for i, template in enumerate(premise_template[:-1]):\n            filled = template.format(A=entity_set[0], B=entity_set[1], C=entity_set[2])\n            premises_text.append(filled)\n        \n        conclusion = premise_template[-1].format(A=entity_set[0], B=entity_set[1], C=entity_set[2])\n        \n        # Sometimes make it false\n        if random.random() < 0.3:\n            conclusion = conclusion.replace(\"are\", \"are not\") if \"are\" in conclusion else conclusion.replace(\"All\", \"No\")\n            correct_answer = \"No\"\n        else:\n            correct_answer = \"Yes\"\n        \n        question = f\"Given: {'. '.join(premises_text)}. Can we conclude: {conclusion}?\"\n        \n        return {\n            \"question\": question,\n            \"answer\": correct_answer,\n            \"type\": \"logical_deduction\",\n            \"difficulty\": \"medium\"\n        }\n    \n    def generate_mathematical_reasoning(self):\n        \"\"\"Math word problems requiring reasoning\"\"\"\n        templates = [\n            {\n                \"template\": \"A train travels {speed1} km/h for {time1} hours, then {speed2} km/h for {time2} hours. What is the average speed?\",\n                \"solution\": lambda v: (v['speed1']*v['time1'] + v['speed2']*v['time2'])/(v['time1']+v['time2'])\n            },\n            {\n                \"template\": \"If {workers1} workers can complete a job in {days1} days, how many days will {workers2} workers take?\",\n                \"solution\": lambda v: (v['workers1']*v['days1'])/v['workers2']\n            },\n            {\n                \"template\": \"A rectangle has perimeter {perimeter} and length {length}. What is its area?\",\n                \"solution\": lambda v: v['length'] * ((v['perimeter']/2) - v['length'])\n            }\n        ]\n        \n        problem = random.choice(templates)\n        \n        # Generate random values\n        if \"speed\" in problem[\"template\"]:\n            values = {\n                \"speed1\": random.randint(40, 80),\n                \"speed2\": random.randint(60, 100),\n                \"time1\": random.randint(2, 5),\n                \"time2\": random.randint(1, 4)\n            }\n        elif \"workers\" in problem[\"template\"]:\n            values = {\n                \"workers1\": random.randint(4, 12),\n                \"days1\": random.randint(10, 30),\n                \"workers2\": random.randint(6, 15)\n            }\n        else:\n            length = random.randint(5, 15)\n            width = random.randint(3, 10)\n            values = {\n                \"perimeter\": 2 * (length + width),\n                \"length\": length\n            }\n        \n        question = problem[\"template\"].format(**values)\n        answer = round(problem[\"solution\"](values), 2)\n        \n        return {\n            \"question\": question,\n            \"answer\": str(answer),\n            \"type\": \"mathematical_reasoning\",\n            \"difficulty\": \"hard\"\n        }\n    \n    def generate_pattern_completion(self):\n        \"\"\"Complex pattern recognition\"\"\"\n        patterns = [\n            {\n                \"name\": \"fibonacci\",\n                \"rule\": lambda seq: seq[-1] + seq[-2],\n                \"init\": [1, 1]\n            },\n            {\n                \"name\": \"geometric\",\n                \"rule\": lambda seq: seq[-1] * 2,\n                \"init\": [2, 4]\n            },\n            {\n                \"name\": \"arithmetic_sequence\",\n                \"rule\": lambda seq: seq[-1] + (seq[-1] - seq[-2]),\n                \"init\": [3, 7]\n            },\n            {\n                \"name\": \"squares_plus_n\",\n                \"rule\": lambda seq: (len(seq)+1)**2 + (len(seq)+1),\n                \"init\": [2, 6]\n            }\n        ]\n        \n        pattern = random.choice(patterns)\n        sequence = pattern[\"init\"].copy()\n        \n        # Generate sequence\n        for _ in range(3):\n            sequence.append(pattern[\"rule\"](sequence))\n        \n        # Hide last element\n        question = f\"What comes next: {', '.join(map(str, sequence[:-1]))}, ?\"\n        answer = str(sequence[-1])\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"type\": \"pattern_completion\",\n            \"pattern\": pattern[\"name\"]\n        }\n    \n    def generate_causal_reasoning(self):\n        \"\"\"Cause and effect reasoning\"\"\"\n        scenarios = [\n            {\n                \"setup\": \"If the temperature drops below freezing and there's moisture in the air\",\n                \"effect\": \"frost will form\",\n                \"negation\": \"frost will not form\"\n            },\n            {\n                \"setup\": \"If a plant doesn't get water for several weeks and is in direct sunlight\",\n                \"effect\": \"the plant will wilt\",\n                \"negation\": \"the plant will thrive\"\n            },\n            {\n                \"setup\": \"If you increase the price of a product significantly and demand is elastic\",\n                \"effect\": \"sales will decrease\",\n                \"negation\": \"sales will increase\"\n            }\n        ]\n        \n        scenario = random.choice(scenarios)\n        \n        # Create variations\n        if random.random() < 0.5:\n            question = f\"{scenario['setup']}, what will happen?\"\n            answer = scenario['effect']\n        else:\n            # Counterfactual\n            question = f\"{scenario['setup']}, will {scenario['negation']}?\"\n            answer = \"No\"\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"type\": \"causal_reasoning\"\n        }\n    \n    def generate_analogical_reasoning(self):\n        \"\"\"Analogies and relationships\"\"\"\n        analogies = [\n            (\"cat\", \"kitten\", \"dog\", \"puppy\"),\n            (\"book\", \"page\", \"house\", \"room\"),\n            (\"teacher\", \"student\", \"doctor\", \"patient\"),\n            (\"key\", \"lock\", \"password\", \"account\"),\n            (\"seed\", \"tree\", \"egg\", \"bird\")\n        ]\n        \n        analogy = random.choice(analogies)\n        question = f\"{analogy[0]} is to {analogy[1]} as {analogy[2]} is to ?\"\n        answer = analogy[3]\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"type\": \"analogical_reasoning\"\n        }\n    \n    def generate_constraint_solving(self):\n        \"\"\"Constraint satisfaction problems\"\"\"\n        people = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"]\n        days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n        \n        # Select subset\n        selected_people = random.sample(people, 3)\n        selected_days = random.sample(days, 3)\n        \n        # Create constraints\n        constraints = []\n        solution = {}\n        \n        # Assign randomly first\n        random.shuffle(selected_days)\n        for i, person in enumerate(selected_people):\n            solution[person] = selected_days[i]\n        \n        # Generate constraints that match solution\n        person1, person2 = random.sample(selected_people, 2)\n        day_index1 = selected_days.index(solution[person1])\n        day_index2 = selected_days.index(solution[person2])\n        \n        if day_index1 < day_index2:\n            constraints.append(f\"{person1} must be before {person2}\")\n        else:\n            constraints.append(f\"{person2} must be before {person1}\")\n        \n        # Add another constraint\n        person = random.choice(selected_people)\n        constraints.append(f\"{person} is on {solution[person]}\")\n        \n        question = f\"Schedule {', '.join(selected_people)} on {', '.join(selected_days)}. Constraints: {'. '.join(constraints)}. When is {random.choice(selected_people)} scheduled?\"\n        answer = solution[random.choice(selected_people)]\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"type\": \"constraint_solving\"\n        }\n    \n    def generate_counterfactual_reasoning(self):\n        \"\"\"What-if scenarios\"\"\"\n        scenarios = [\n            {\n                \"fact\": \"The meeting was cancelled because the presenter was sick\",\n                \"counterfactual\": \"If the presenter hadn't been sick\",\n                \"result\": \"the meeting would have proceeded as scheduled\"\n            },\n            {\n                \"fact\": \"The plant died because it wasn't watered\",\n                \"counterfactual\": \"If the plant had been watered regularly\",\n                \"result\": \"the plant would have survived\"\n            },\n            {\n                \"fact\": \"The team lost because their best player was injured\",\n                \"counterfactual\": \"If their best player hadn't been injured\",\n                \"result\": \"the team might have won\"\n            }\n        ]\n        \n        scenario = random.choice(scenarios)\n        question = f\"{scenario['fact']}. {scenario['counterfactual']}, what would have happened?\"\n        answer = scenario['result']\n        \n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"type\": \"counterfactual_reasoning\"\n        }\n    \n    def generate_task(self):\n        \"\"\"Generate a random task\"\"\"\n        return random.choice(self.task_types)()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.864928Z","iopub.execute_input":"2025-07-25T13:41:21.865232Z","iopub.status.idle":"2025-07-25T13:41:21.887200Z","shell.execute_reply.started":"2025-07-25T13:41:21.865207Z","shell.execute_reply":"2025-07-25T13:41:21.886602Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class PreferenceDataGenerator:\n    \"\"\"Generate preference pairs for DPO training using local model\"\"\"\n    \n    def __init__(self, model, tokenizer, task_generator, config):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.task_generator = task_generator\n        self.config = config\n        \n        self.system_prompt = \"\"\"You are a reasoning assistant. When solving problems:\n1. Think step-by-step in <think> tags\n2. Show your work clearly\n3. Provide the final answer in <answer> tags\n\nFormat:\n<think>\n[Step-by-step reasoning]\n</think>\n<answer>\n[Final answer]\n</answer>\"\"\"\n    \n    def extract_answer(self, response: str) -> str:\n        \"\"\"Extract answer from response\"\"\"\n        match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n        if match:\n            return match.group(1).strip()\n        return \"\"\n    \n    def score_response(self, task: Dict, response: str) -> Dict[str, float]:\n        \"\"\"Score a response on multiple criteria\"\"\"\n        scores = {}\n        \n        # 1. Correctness (most important)\n        answer = self.extract_answer(response)\n        correct_answer = task[\"answer\"].lower().strip()\n        extracted_answer = answer.lower().strip()\n        \n        # Flexible matching for different answer types\n        if task[\"type\"] == \"mathematical_reasoning\":\n            try:\n                scores[\"correctness\"] = 1.0 if abs(float(extracted_answer) - float(correct_answer)) < 0.01 else 0.0\n            except:\n                scores[\"correctness\"] = 0.0\n        else:\n            scores[\"correctness\"] = 1.0 if extracted_answer == correct_answer else 0.0\n        \n        # 2. Format compliance\n        has_think = \"<think>\" in response and \"</think>\" in response\n        has_answer = \"<answer>\" in response and \"</answer>\" in response\n        proper_order = response.find(\"<think>\") < response.find(\"</think>\") < response.find(\"<answer>\") < response.find(\"</answer>\") if has_think and has_answer else False\n        scores[\"format\"] = 1.0 if proper_order else 0.0\n        \n        # 3. Reasoning quality (heuristic)\n        think_match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n        if think_match:\n            reasoning = think_match.group(1)\n            # Check for step-by-step indicators\n            step_indicators = [\"first\", \"second\", \"then\", \"therefore\", \"because\", \"step\", \"next\"]\n            step_count = sum(1 for indicator in step_indicators if indicator in reasoning.lower())\n            scores[\"reasoning_quality\"] = min(step_count / 3.0, 1.0)  # Normalize\n        else:\n            scores[\"reasoning_quality\"] = 0.0\n        \n        # 4. Length penalty (prefer concise but complete)\n        response_length = len(response.split())\n        if response_length < 20:\n            scores[\"length\"] = 0.5  # Too short\n        elif response_length > 200:\n            scores[\"length\"] = 0.7  # Too long\n        else:\n            scores[\"length\"] = 1.0  # Just right\n        \n        # Calculate total score\n        weights = {\n            \"correctness\": 0.5,\n            \"format\": 0.2,\n            \"reasoning_quality\": 0.2,\n            \"length\": 0.1\n        }\n        \n        scores[\"total\"] = sum(scores[k] * weights[k] for k in weights.keys())\n        \n        return scores\n    \n    def generate_response_local(self, task: Dict) -> str:\n        \"\"\"Generate response using local model\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": task[\"question\"]}\n        ]\n        \n        inputs = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        ).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_new_tokens=self.config.generation_max_tokens,\n                temperature=self.config.generation_temperature,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n        return response\n    \n    def create_preference_pairs(self, task: Dict, responses: List[str]) -> List[Dict]:\n        \"\"\"Create preference pairs from responses\"\"\"\n        # Score all responses\n        scored_responses = []\n        for response in responses:\n            scores = self.score_response(task, response)\n            scored_responses.append({\n                \"response\": response,\n                \"scores\": scores,\n                \"total_score\": scores[\"total\"]\n            })\n        \n        # Sort by total score\n        scored_responses.sort(key=lambda x: x[\"total_score\"], reverse=True)\n        \n        # Create preference pairs\n        preference_pairs = []\n        \n        # Pair top half with bottom half for preferences\n        if len(scored_responses) >= 2:\n            for i in range(len(scored_responses) // 2):\n                for j in range(len(scored_responses) // 2, len(scored_responses)):\n                    if scored_responses[i][\"total_score\"] > scored_responses[j][\"total_score\"]:\n                        preference_pairs.append({\n                            \"prompt\": task[\"question\"],\n                            \"chosen\": scored_responses[i][\"response\"],\n                            \"rejected\": scored_responses[j][\"response\"],\n                            \"chosen_score\": scored_responses[i][\"total_score\"],\n                            \"rejected_score\": scored_responses[j][\"total_score\"],\n                            \"task_type\": task[\"type\"],\n                            \"metadata\": {\n                                \"correct_answer\": task[\"answer\"],\n                                \"chosen_scores\": scored_responses[i][\"scores\"],\n                                \"rejected_scores\": scored_responses[j][\"scores\"]\n                            }\n                        })\n        \n        return preference_pairs\n    \n    def generate_preference_dataset(self, num_tasks: int) -> List[Dict]:\n        \"\"\"Generate complete preference dataset\"\"\"\n        all_preferences = []\n        \n        print(f\"Generating preference data for {num_tasks} tasks...\")\n        \n        for i in tqdm(range(num_tasks)):\n            # Generate task\n            task = self.task_generator.generate_task()\n            \n            # Generate responses using local model\n            responses = [self.generate_response_local(task) for _ in range(self.config.responses_per_prompt)]\n            \n            # Create preference pairs\n            pairs = self.create_preference_pairs(task, responses)\n            all_preferences.extend(pairs)\n            \n            # Clear memory periodically\n            if i % 50 == 0:\n                torch.cuda.empty_cache()\n                gc.collect()\n        \n        print(f\"Generated {len(all_preferences)} preference pairs from {num_tasks} tasks\")\n        \n        return all_preferences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.888593Z","iopub.execute_input":"2025-07-25T13:41:21.888827Z","iopub.status.idle":"2025-07-25T13:41:21.911555Z","shell.execute_reply.started":"2025-07-25T13:41:21.888811Z","shell.execute_reply":"2025-07-25T13:41:21.910795Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class ReasoningPreferenceDataset(Dataset):\n    \"\"\"Dataset for DPO training\"\"\"\n    \n    def __init__(self, preferences: List[Dict], tokenizer, max_length: int = 512):\n        self.preferences = preferences\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # System prompt (same for all)\n        self.system_prompt = \"\"\"You are a reasoning assistant. When solving problems:\n1. Think step-by-step in <think> tags\n2. Show your work clearly\n3. Provide the final answer in <answer> tags\"\"\"\n    \n    def __len__(self):\n        return len(self.preferences)\n    \n    def __getitem__(self, idx):\n        preference = self.preferences[idx]\n        \n        # Format prompt\n        prompt = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": preference[\"prompt\"]}\n        ]\n        \n        # Tokenize prompt\n        prompt_tokens = self.tokenizer.apply_chat_template(\n            prompt,\n            tokenize=True,\n            add_generation_prompt=True\n        )\n        \n        # Tokenize chosen response\n        chosen_tokens = self.tokenizer.encode(\n            preference[\"chosen\"],\n            add_special_tokens=False\n        )\n        \n        # Tokenize rejected response\n        rejected_tokens = self.tokenizer.encode(\n            preference[\"rejected\"],\n            add_special_tokens=False\n        )\n        \n        # Combine and pad\n        chosen_input_ids = prompt_tokens + chosen_tokens\n        rejected_input_ids = prompt_tokens + rejected_tokens\n        \n        # Truncate if needed\n        if len(chosen_input_ids) > self.max_length:\n            chosen_input_ids = chosen_input_ids[:self.max_length]\n        if len(rejected_input_ids) > self.max_length:\n            rejected_input_ids = rejected_input_ids[:self.max_length]\n        \n        return {\n            \"prompt\": preference[\"prompt\"],\n            \"chosen\": preference[\"chosen\"],\n            \"rejected\": preference[\"rejected\"],\n            \"chosen_input_ids\": chosen_input_ids,\n            \"rejected_input_ids\": rejected_input_ids,\n            \"prompt_length\": len(prompt_tokens),\n            \"metadata\": preference.get(\"metadata\", {})\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.912315Z","iopub.execute_input":"2025-07-25T13:41:21.912509Z","iopub.status.idle":"2025-07-25T13:41:21.934134Z","shell.execute_reply.started":"2025-07-25T13:41:21.912494Z","shell.execute_reply":"2025-07-25T13:41:21.933480Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class ReasonDPOTrainer:\n    \"\"\"Custom DPO trainer with reasoning-specific features\"\"\"\n    \n    def __init__(\n        self,\n        model,\n        ref_model,\n        tokenizer,\n        train_dataset,\n        eval_dataset,\n        config: ReasonDPOConfig\n    ):\n        self.model = model\n        self.ref_model = ref_model\n        self.tokenizer = tokenizer\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.config = config\n        \n        # Setup optimizer\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            betas=(0.9, 0.999),\n            weight_decay=0.01\n        )\n        \n        # Setup scheduler\n        total_steps = len(train_dataset) * config.num_epochs // (config.batch_size * config.gradient_accumulation_steps)\n        self.scheduler = get_linear_schedule_with_warmup(\n            self.optimizer,\n            num_warmup_steps=int(0.1 * total_steps),\n            num_training_steps=total_steps\n        )\n        \n        # Metrics\n        self.train_losses = []\n        self.eval_metrics = []\n    \n    def compute_dpo_loss(self, batch):\n        \"\"\"Compute DPO loss for a batch\"\"\"\n        # Get model outputs for chosen and rejected\n        chosen_outputs = self.model(\n            input_ids=batch[\"chosen_input_ids\"],\n            attention_mask=batch[\"chosen_attention_mask\"]\n        )\n        rejected_outputs = self.model(\n            input_ids=batch[\"rejected_input_ids\"],\n            attention_mask=batch[\"rejected_attention_mask\"]\n        )\n        \n        # Get reference model outputs\n        with torch.no_grad():\n            ref_chosen_outputs = self.ref_model(\n                input_ids=batch[\"chosen_input_ids\"],\n                attention_mask=batch[\"chosen_attention_mask\"]\n            )\n            ref_rejected_outputs = self.ref_model(\n                input_ids=batch[\"rejected_input_ids\"],\n                attention_mask=batch[\"rejected_attention_mask\"]\n            )\n        \n        # Calculate log probabilities\n        chosen_logprobs = self.get_batch_logprobs(\n            chosen_outputs.logits,\n            batch[\"chosen_input_ids\"],\n            batch[\"chosen_attention_mask\"],\n            batch[\"prompt_length\"]\n        )\n        \n        rejected_logprobs = self.get_batch_logprobs(\n            rejected_outputs.logits,\n            batch[\"rejected_input_ids\"],\n            batch[\"rejected_attention_mask\"],\n            batch[\"prompt_length\"]\n        )\n        \n        ref_chosen_logprobs = self.get_batch_logprobs(\n            ref_chosen_outputs.logits,\n            batch[\"chosen_input_ids\"],\n            batch[\"chosen_attention_mask\"],\n            batch[\"prompt_length\"]\n        )\n        \n        ref_rejected_logprobs = self.get_batch_logprobs(\n            ref_rejected_outputs.logits,\n            batch[\"rejected_input_ids\"],\n            batch[\"rejected_attention_mask\"],\n            batch[\"prompt_length\"]\n        )\n        \n        # DPO loss\n        chosen_rewards = self.config.beta * (chosen_logprobs - ref_chosen_logprobs)\n        rejected_rewards = self.config.beta * (rejected_logprobs - ref_rejected_logprobs)\n        \n        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n        \n        # Metrics\n        metrics = {\n            \"loss\": loss.item(),\n            \"chosen_rewards\": chosen_rewards.mean().item(),\n            \"rejected_rewards\": rejected_rewards.mean().item(),\n            \"reward_margin\": (chosen_rewards - rejected_rewards).mean().item(),\n            \"reward_accuracy\": ((chosen_rewards > rejected_rewards).float().mean().item())\n        }\n        \n        return loss, metrics\n    \n    def get_batch_logprobs(self, logits, input_ids, attention_mask, prompt_lengths):\n        \"\"\"Calculate log probabilities for responses\"\"\"\n        batch_size = logits.shape[0]\n        logprobs = []\n        \n        for i in range(batch_size):\n            # Get response portion\n            prompt_len = prompt_lengths[i]\n            response_logits = logits[i, prompt_len-1:-1]\n            response_ids = input_ids[i, prompt_len:]\n            response_mask = attention_mask[i, prompt_len:]\n            \n            # Calculate log probs\n            log_probs = F.log_softmax(response_logits, dim=-1)\n            selected_log_probs = log_probs.gather(\n                dim=-1,\n                index=response_ids.unsqueeze(-1)\n            ).squeeze(-1)\n            \n            # Mask and sum\n            masked_log_probs = selected_log_probs * response_mask\n            total_log_prob = masked_log_probs.sum() / response_mask.sum()\n            \n            logprobs.append(total_log_prob)\n        \n        return torch.stack(logprobs)\n    \n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        epoch_losses = []\n        epoch_metrics = {\n            \"reward_margin\": [],\n            \"reward_accuracy\": []\n        }\n        \n        # Create dataloader\n        train_dataloader = DataLoader(\n            self.train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            collate_fn=self.collate_fn\n        )\n        \n        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n        \n        for step, batch in enumerate(pbar):\n            # Move to device\n            batch = {k: v.to(self.model.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n            \n            # Forward pass\n            loss, metrics = self.compute_dpo_loss(batch)\n            \n            # Backward pass\n            loss = loss / self.config.gradient_accumulation_steps\n            loss.backward()\n            \n            # Update weights\n            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n            \n            # Track metrics\n            epoch_losses.append(metrics[\"loss\"])\n            epoch_metrics[\"reward_margin\"].append(metrics[\"reward_margin\"])\n            epoch_metrics[\"reward_accuracy\"].append(metrics[\"reward_accuracy\"])\n            \n            # Update progress bar\n            pbar.set_postfix({\n                \"loss\": f\"{metrics['loss']:.4f}\",\n                \"margin\": f\"{metrics['reward_margin']:.3f}\",\n                \"acc\": f\"{metrics['reward_accuracy']:.2%}\"\n            })\n        \n        # Epoch summary\n        avg_loss = np.mean(epoch_losses)\n        avg_margin = np.mean(epoch_metrics[\"reward_margin\"])\n        avg_accuracy = np.mean(epoch_metrics[\"reward_accuracy\"])\n        \n        print(f\"\\nEpoch {epoch+1} Summary:\")\n        print(f\"  Average Loss: {avg_loss:.4f}\")\n        print(f\"  Average Reward Margin: {avg_margin:.3f}\")\n        print(f\"  Average Reward Accuracy: {avg_accuracy:.2%}\")\n        \n        return avg_loss, avg_margin, avg_accuracy\n    \n    def evaluate(self):\n        \"\"\"Evaluate on validation set\"\"\"\n        self.model.eval()\n        eval_losses = []\n        eval_margins = []\n        eval_accuracies = []\n        \n        eval_dataloader = DataLoader(\n            self.eval_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            collate_fn=self.collate_fn\n        )\n        \n        with torch.no_grad():\n            for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n                batch = {k: v.to(self.model.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n                \n                loss, metrics = self.compute_dpo_loss(batch)\n                \n                eval_losses.append(metrics[\"loss\"])\n                eval_margins.append(metrics[\"reward_margin\"])\n                eval_accuracies.append(metrics[\"reward_accuracy\"])\n        \n        avg_eval_loss = np.mean(eval_losses)\n        avg_eval_margin = np.mean(eval_margins)\n        avg_eval_accuracy = np.mean(eval_accuracies)\n        \n        print(f\"\\nEvaluation Results:\")\n        print(f\"  Loss: {avg_eval_loss:.4f}\")\n        print(f\"  Reward Margin: {avg_eval_margin:.3f}\")\n        print(f\"  Reward Accuracy: {avg_eval_accuracy:.2%}\")\n        \n        return avg_eval_loss, avg_eval_margin, avg_eval_accuracy\n    \n    def collate_fn(self, batch):\n        \"\"\"Custom collate function for DPO\"\"\"\n        # Pad sequences\n        chosen_input_ids = [item[\"chosen_input_ids\"] for item in batch]\n        rejected_input_ids = [item[\"rejected_input_ids\"] for item in batch]\n        prompt_lengths = [item[\"prompt_length\"] for item in batch]\n        \n        # Pad\n        chosen_padded = self.tokenizer.pad(\n            {\"input_ids\": chosen_input_ids},\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        \n        rejected_padded = self.tokenizer.pad(\n            {\"input_ids\": rejected_input_ids},\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"chosen_input_ids\": chosen_padded[\"input_ids\"],\n            \"chosen_attention_mask\": chosen_padded[\"attention_mask\"],\n            \"rejected_input_ids\": rejected_padded[\"input_ids\"],\n            \"rejected_attention_mask\": rejected_padded[\"attention_mask\"],\n            \"prompt_length\": torch.tensor(prompt_lengths)\n        }\n    \n    def train(self):\n        \"\"\"Main training loop\"\"\"\n        print(\"🚀 Starting DPO Training\")\n        \n        best_eval_loss = float('inf')\n        \n        for epoch in range(self.config.num_epochs):\n            # Train\n            train_loss, train_margin, train_acc = self.train_epoch(epoch)\n            \n            # Evaluate\n            eval_loss, eval_margin, eval_acc = self.evaluate()\n            \n            # Save best model\n            if eval_loss < best_eval_loss:\n                best_eval_loss = eval_loss\n                self.save_model(f\"{self.config.output_dir}/best_model\")\n                print(f\"✅ Saved best model with eval loss: {eval_loss:.4f}\")\n            \n            # Save checkpoint\n            self.save_model(f\"{self.config.output_dir}/checkpoint-epoch-{epoch+1}\")\n            \n            # Clear memory\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        print(\"\\n🎉 Training Complete!\")\n        \n        return self.model\n    \n    def save_model(self, path):\n        \"\"\"Save model and tokenizer\"\"\"\n        os.makedirs(path, exist_ok=True)\n        self.model.save_pretrained(path)\n        self.tokenizer.save_pretrained(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.935326Z","iopub.execute_input":"2025-07-25T13:41:21.935547Z","iopub.status.idle":"2025-07-25T13:41:21.958282Z","shell.execute_reply.started":"2025-07-25T13:41:21.935530Z","shell.execute_reply":"2025-07-25T13:41:21.957578Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_reason_dpo(config: ReasonDPOConfig):\n    \"\"\"Main training pipeline\"\"\"\n    \n    print(\"Starting ReasonDPO Training Pipeline\")\n    \n    # Load models\n    print(\"Loading models...\")\n    \n    # Quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        config.model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    # Load reference model (frozen)\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        config.ref_model_name or config.model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    ref_model.eval()\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"left\"  # Important for batch generation\n    \n    # Prepare model for training\n    model = prepare_model_for_kbit_training(model)\n    model.gradient_checkpointing_enable()\n    \n    # Add LoRA\n    peft_config = LoraConfig(\n        r=config.lora_r,\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    )\n    \n    model = get_peft_model(model, peft_config)\n    trainable_params, all_params = model.get_nb_trainable_parameters()\n    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n    \n    # Generate preference data\n    print(\"Generating preference data...\")\n    task_generator = AdvancedReasoningTasks()\n    preference_generator = PreferenceDataGenerator(model, tokenizer, task_generator, config)\n    \n    # Generate datasets\n    train_preferences = preference_generator.generate_preference_dataset(config.num_train_preferences)\n    eval_preferences = preference_generator.generate_preference_dataset(config.num_eval_preferences)\n    \n    # Save preference data\n    os.makedirs(config.output_dir, exist_ok=True)\n    with open(f\"{config.output_dir}/train_preferences.json\", \"w\") as f:\n        json.dump(train_preferences, f, indent=2)\n    with open(f\"{config.output_dir}/eval_preferences.json\", \"w\") as f:\n        json.dump(eval_preferences, f, indent=2)\n    \n    # Create datasets\n    train_dataset = ReasoningPreferenceDataset(train_preferences, tokenizer, config.max_length)\n    eval_dataset = ReasoningPreferenceDataset(eval_preferences, tokenizer, config.max_length)\n    \n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Eval dataset size: {len(eval_dataset)}\")\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=config.output_dir,\n        per_device_train_batch_size=config.batch_size,\n        per_device_eval_batch_size=config.batch_size,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        learning_rate=config.learning_rate,\n        num_train_epochs=config.num_epochs,\n        bf16=True,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        optim=\"adamw_torch\",\n        gradient_checkpointing=True,\n        warmup_steps=100,\n        report_to=\"none\",  # Can add wandb if needed\n    )\n    \n    # Create TRL DPO trainer\n    trainer = DPOTrainer(\n        model=model,\n        ref_model=ref_model,\n        args=training_args,\n        beta=config.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        max_length=config.max_length,\n        max_prompt_length=config.max_prompt_length,\n        peft_config=peft_config,\n    )\n    \n    # Train\n    trainer.train()\n    \n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.984024Z","iopub.execute_input":"2025-07-25T13:41:21.984759Z","iopub.status.idle":"2025-07-25T13:41:21.994344Z","shell.execute_reply.started":"2025-07-25T13:41:21.984739Z","shell.execute_reply":"2025-07-25T13:41:21.993585Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def evaluate_reasoning_capability(model, tokenizer, num_samples=50):\n    \"\"\"Comprehensive evaluation of reasoning capabilities\"\"\"\n    \n    print(\"Evaluating Reasoning Capabilities...\")\n    \n    task_generator = AdvancedReasoningTasks()\n    results = {task_type: {\"correct\": 0, \"total\": 0} for task_type in [\n        \"logical_deduction\", \"mathematical_reasoning\", \"pattern_completion\",\n        \"causal_reasoning\", \"analogical_reasoning\", \"constraint_solving\",\n        \"counterfactual_reasoning\"\n    ]}\n    \n    system_prompt = \"\"\"You are a reasoning assistant. When solving problems:\n1. Think step-by-step in <think> tags\n2. Show your work clearly\n3. Provide the final answer in <answer> tags\"\"\"\n    \n    for _ in tqdm(range(num_samples), desc=\"Evaluating\"):\n        # Generate task\n        task = task_generator.generate_task()\n        task_type = task[\"type\"]\n        \n        # Generate response\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": task[\"question\"]}\n        ]\n        \n        inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        ).to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                inputs,\n                max_new_tokens=300,\n                temperature=0.3,  # Lower for evaluation\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n        \n        # Extract and check answer\n        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n        if answer_match:\n            generated_answer = answer_match.group(1).strip()\n            correct_answer = task[\"answer\"]\n            \n            # Check correctness\n            if task_type == \"mathematical_reasoning\":\n                try:\n                    is_correct = abs(float(generated_answer) - float(correct_answer)) < 0.01\n                except:\n                    is_correct = False\n            else:\n                is_correct = generated_answer.lower() == correct_answer.lower()\n            \n            if is_correct:\n                results[task_type][\"correct\"] += 1\n        \n        results[task_type][\"total\"] += 1\n    \n    # Calculate accuracies\n    print(\"Results by Task Type:\")\n    overall_correct = 0\n    overall_total = 0\n    \n    for task_type, stats in results.items():\n        if stats[\"total\"] > 0:\n            accuracy = stats[\"correct\"] / stats[\"total\"]\n            print(f\"  {task_type}: {accuracy:.2%} ({stats['correct']}/{stats['total']})\")\n            overall_correct += stats[\"correct\"]\n            overall_total += stats[\"total\"]\n    \n    overall_accuracy = overall_correct / overall_total if overall_total > 0 else 0\n    print(f\"Overall Accuracy: {overall_accuracy:.2%}\")\n    \n    return results, overall_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:21.995688Z","iopub.execute_input":"2025-07-25T13:41:21.996113Z","iopub.status.idle":"2025-07-25T13:41:22.015963Z","shell.execute_reply.started":"2025-07-25T13:41:21.996096Z","shell.execute_reply":"2025-07-25T13:41:22.015344Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def online_dpo_iteration(model, tokenizer, config, iteration=1):\n    \"\"\"One iteration of online DPO for self-improvement\"\"\"\n    \n    print(f\"Online DPO Iteration {iteration}\")\n    \n    # Use current model to generate new preference data\n    task_generator = AdvancedReasoningTasks()\n    preference_generator = PreferenceDataGenerator(model, tokenizer, task_generator, config)\n    \n    # Generate smaller dataset for online learning\n    online_config = ReasonDPOConfig(\n        num_train_preferences=1000,\n        num_eval_preferences=200,\n        num_epochs=1,\n        learning_rate=config.learning_rate * 0.5  # Lower LR for fine-tuning\n    )\n    \n    # Generate new preferences with current model\n    train_preferences = preference_generator.generate_preference_dataset(online_config.num_train_preferences)\n    eval_preferences = preference_generator.generate_preference_dataset(online_config.num_eval_preferences)\n    \n    # Filter for high-quality pairs (larger score differences)\n    train_preferences = [\n        p for p in train_preferences \n        if p[\"chosen_score\"] - p[\"rejected_score\"] > 0.3\n    ]\n    \n    print(f\"Filtered to {len(train_preferences)} high-quality preference pairs\")\n    \n    # Create datasets\n    train_dataset = ReasoningPreferenceDataset(train_preferences, tokenizer, config.max_length)\n    eval_dataset = ReasoningPreferenceDataset(eval_preferences, tokenizer, config.max_length)\n    \n    # Training arguments for online iteration\n    training_args = TrainingArguments(\n        output_dir=config.output_dir,\n        per_device_train_batch_size=config.batch_size,\n        per_device_eval_batch_size=config.batch_size,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        learning_rate=online_config.learning_rate,\n        num_train_epochs=online_config.num_epochs,\n        bf16=True,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        optim=\"adamw_torch\",\n        gradient_checkpointing=True,\n        warmup_steps=50,\n        report_to=\"none\",\n    )\n    \n    # Create new DPO trainer with current model as both model and reference\n    trainer = DPOTrainer(\n        model=model,\n        ref_model=model,  # Self as reference for online DPO\n        args=training_args,\n        beta=config.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        max_length=config.max_length,\n        max_prompt_length=config.max_prompt_length,\n        peft_config=None,  # LoRA already applied\n    )\n    \n    # Train\n    trainer.train()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:41:22.016874Z","iopub.execute_input":"2025-07-25T13:41:22.017132Z","iopub.status.idle":"2025-07-25T13:41:22.036743Z","shell.execute_reply.started":"2025-07-25T13:41:22.017110Z","shell.execute_reply":"2025-07-25T13:41:22.036227Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def main():\n    \"\"\"Main execution function\"\"\"\n    \n    # Initial DPO training\n    trained_model, tokenizer = train_reason_dpo(config)\n    \n    # Evaluate initial performance\n    print(\"Initial Model Evaluation:\")\n    initial_results, initial_accuracy = evaluate_reasoning_capability(\n        trained_model, tokenizer, num_samples=100\n    )\n    \n    # Online DPO iterations\n    current_model = trained_model\n    for i in range(1):  # 3 online iterations\n        print(f\"Starting Online DPO Iteration {i+1}\")\n        \n        # Self-improvement\n        current_model = online_dpo_iteration(\n            current_model, tokenizer, config, iteration=i+1\n        )\n        \n        # Evaluate improvement\n        results, accuracy = evaluate_reasoning_capability(\n            current_model, tokenizer, num_samples=50\n        )\n        \n        print(f\"Accuracy after iteration {i+1}: {accuracy:.2%}\")\n        \n        # Save checkpoint\n        save_path = f\"{config.output_dir}/online_dpo_iteration_{i+1}\"\n        current_model.save_pretrained(save_path)\n        tokenizer.save_pretrained(save_path)\n    \n    # Final evaluation\n    print(\"Final Model Evaluation:\")\n    final_results, final_accuracy = evaluate_reasoning_capability(\n        current_model, tokenizer, num_samples=100\n    )\n    \n    print(\"Training Complete!\")\n    print(f\"Initial Accuracy: {initial_accuracy:.2%}\")\n    print(f\"Final Accuracy: {final_accuracy:.2%}\")\n    print(f\"Improvement: {final_accuracy - initial_accuracy:.2%}\")\n    \n    # Save final model\n    final_path = f\"{config.output_dir}/final_model\"\n    current_model.save_pretrained(final_path)\n    tokenizer.save_pretrained(final_path)\n    \n    return current_model, tokenizer\n\n# Run training\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:44:48.097854Z","iopub.execute_input":"2025-07-25T13:44:48.098601Z","iopub.status.idle":"2025-07-25T19:15:23.466737Z","shell.execute_reply.started":"2025-07-25T13:44:48.098574Z","shell.execute_reply":"2025-07-25T19:15:23.464942Z"}},"outputs":[{"name":"stdout","text":"Starting ReasonDPO Training Pipeline\nLoading models...\nTrainable parameters: 8,683,520 (2.34%)\nGenerating preference data...\nGenerating preference data for 200 tasks...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [4:34:21<00:00, 82.31s/it]   \n","output_type":"stream"},{"name":"stdout","text":"Generated 316 preference pairs from 200 tasks\nGenerating preference data for 40 tasks...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [56:08<00:00, 84.21s/it]  \n/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated 87 preference pairs from 40 tasks\nTrain dataset size: 316\nEval dataset size: 87\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_168/2877963866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_168/2877963866.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Initial DPO training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_reason_dpo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Evaluate initial performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_168/2259061202.py\u001b[0m in \u001b[0;36mtrain_reason_dpo\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Create TRL DPO trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     trainer = DPOTrainer(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mref_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/dpo_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, ref_model, beta, label_smoothing, loss_type, args, data_collator, label_pad_token_id, padding_value, truncation_mode, train_dataset, eval_dataset, tokenizer, model_init, callbacks, optimizers, preprocess_logits_for_metrics, max_length, max_prompt_length, max_target_length, peft_config, is_encoder_decoder, disable_dropout, generate_during_eval, compute_metrics, precompute_ref_log_probs, dataset_num_proc, model_init_kwargs, ref_model_init_kwargs, model_adapter_name, ref_adapter_name, reference_free, force_use_ref_model)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mref_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce_use_ref_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;34m\"You passed both a ref_model and a peft_config. For training PEFT adapters with DPO there is no need to pass a reference\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;34m\" model. Please pass `ref_model=None` in case you want to train PEFT adapters, or pass a ref_model with `force_use_ref_model=True` in DPOTrainer's init.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You passed both a ref_model and a peft_config. For training PEFT adapters with DPO there is no need to pass a reference model. Please pass `ref_model=None` in case you want to train PEFT adapters, or pass a ref_model with `force_use_ref_model=True` in DPOTrainer's init. if you want to use a different ref_model."],"ename":"ValueError","evalue":"You passed both a ref_model and a peft_config. For training PEFT adapters with DPO there is no need to pass a reference model. Please pass `ref_model=None` in case you want to train PEFT adapters, or pass a ref_model with `force_use_ref_model=True` in DPOTrainer's init. if you want to use a different ref_model.","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"def interactive_reasoning_demo(model, tokenizer):\n    \"\"\"Interactive demo for testing the model\"\"\"\n    \n    print(\"Interactive Reasoning Demo\")\n    print(\"Type 'quit' to exit, 'examples' for sample questions\")\n    \n    examples = [\n        \"What comes next in the sequence: 3, 8, 15, 24, 35, ?\",\n        \"If all birds can fly and penguins are birds, can penguins fly? What's wrong with this logic?\",\n        \"A train travels 60 km/h for 2 hours, then 80 km/h for 3 hours. What's the average speed?\",\n        \"If the meeting was cancelled because of rain, what would have happened if it hadn't rained?\",\n        \"Book is to page as house is to ?\",\n        \"Arrange Alice, Bob, Charlie so that Alice is before Bob and Charlie is not last. List all valid arrangements.\"\n    ]\n    \n    system_prompt = \"\"\"You are a reasoning assistant. When solving problems:\n1. Think step-by-step in <think> tags\n2. Show your work clearly\n3. Provide the final answer in <answer> tags\"\"\"\n    \n    while True:\n        user_input = input(\"Enter your reasoning question: \")\n        \n        if user_input.lower() == 'quit':\n            break\n        elif user_input.lower() == 'examples':\n            print(\"Example questions:\")\n            for i, ex in enumerate(examples, 1):\n                print(f\"{i}. {ex}\")\n            continue\n        \n        # Generate response\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_input}\n        ]\n        \n        inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        ).to(model.device)\n        \n        print(\"Thinking...\")\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                inputs,\n                max_new_tokens=400,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n        \n        # Parse and display response\n        think_match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n        \n        if think_match:\n            print(\"Reasoning Process:\")\n            print(think_match.group(1).strip())\n        \n        if answer_match:\n            print(\"Answer:\")\n            print(answer_match.group(1).strip())\n        \n        if not think_match and not answer_match:\n            print(\"Response:\")\n            print(response)\n\n# Run demo with trained model (comment out if not needed)\n# interactive_reasoning_demo(trained_model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T13:44:27.868349Z","iopub.status.idle":"2025-07-25T13:44:27.868680Z","shell.execute_reply.started":"2025-07-25T13:44:27.868511Z","shell.execute_reply":"2025-07-25T13:44:27.868527Z"}},"outputs":[],"execution_count":null}]}