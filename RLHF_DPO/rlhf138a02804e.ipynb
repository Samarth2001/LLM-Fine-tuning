{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "execution_failed": "2025-07-25T15:38:47.691Z",
     "iopub.execute_input": "2025-07-25T15:36:26.527542Z",
     "iopub.status.busy": "2025-07-25T15:36:26.526920Z"
    },
    "id": "OMxJT0MtkVLK",
    "outputId": "6650dc67-bcac-4948-9b5e-53a28ed233ab",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.52.4\n",
      "Uninstalling transformers-4.52.4:\n",
      "  Successfully uninstalled transformers-4.52.4\n",
      "Found existing installation: accelerate 1.8.1\n",
      "Uninstalling accelerate-1.8.1:\n",
      "  Successfully uninstalled accelerate-1.8.1\n",
      "Found existing installation: peft 0.15.2\n",
      "Uninstalling peft-0.15.2:\n",
      "  Successfully uninstalled peft-0.15.2\n",
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: datasets 3.6.0\n",
      "Uninstalling datasets-3.6.0:\n",
      "  Successfully uninstalled datasets-3.6.0\n",
      "\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: scipy 1.15.3\n",
      "Uninstalling scipy-1.15.3:\n",
      "  Successfully uninstalled scipy-1.15.3\n",
      "Found existing installation: triton 3.2.0\n",
      "Uninstalling triton-3.2.0:\n",
      "  Successfully uninstalled triton-3.2.0\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtune 0.6.1 requires datasets, which is not installed.\n",
      "kaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\n",
      "sentence-transformers 4.1.0 requires scipy, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
      "fastai 2.7.19 requires scipy, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
      "cesium 0.12.4 requires scipy>=0.16.0, which is not installed.\n",
      "sentence-transformers 4.1.0 requires scipy, which is not installed.\n",
      "fastai 2.7.19 requires scipy, which is not installed.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting triton==3.2.0 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Installing collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "Successfully installed triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers accelerate peft bitsandbytes datasets trl scipy triton\n",
    "\n",
    "!pip install --upgrade transformers==4.41.2 -q\n",
    "!pip install --upgrade peft==0.11.1 -q\n",
    "!pip install --upgrade accelerate==0.30.1 -q\n",
    "!pip install bitsandbytes -q\n",
    "!pip install --upgrade datasets==2.19.1 -q\n",
    "!pip install --upgrade trl==0.8.6 -q\n",
    "!pip install --upgrade scipy -q\n",
    "!pip install --upgrade triton -q\n",
    "\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q einops\n",
    "\n",
    "# Install bitsandbytes from alternative source for better compatibility with colab\n",
    "!pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n",
    "# Restart runtime to apply changes\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:39:43.243889Z",
     "iopub.status.busy": "2025-07-25T15:39:43.243094Z",
     "iopub.status.idle": "2025-07-25T15:39:43.249958Z",
     "shell.execute_reply": "2025-07-25T15:39:43.249408Z",
     "shell.execute_reply.started": "2025-07-25T15:39:43.243861Z"
    },
    "id": "_HPEdxmEr6I0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:39:43.251331Z",
     "iopub.status.busy": "2025-07-25T15:39:43.251082Z",
     "iopub.status.idle": "2025-07-25T15:40:04.156009Z",
     "shell.execute_reply": "2025-07-25T15:40:04.155415Z",
     "shell.execute_reply.started": "2025-07-25T15:39:43.251313Z"
    },
    "id": "yCwMt7LAk5N7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 15:39:53.241856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753457993.436186     163 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753457993.497003     163 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class SmartReasonConfig:\n",
    "    # Model settings\n",
    "    model_name: str = \"HuggingfaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "    # Training settings\n",
    "    learning_rate: float = 1e-5\n",
    "    batch_size: int = 32\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    num_epochs: int = 2\n",
    "    warmup_steps: int = 20\n",
    "\n",
    "    # GRPO settings\n",
    "    num_rollouts: int = 2\n",
    "    buffer_size: int = 512\n",
    "    ppo_epochs: int = 3\n",
    "    clip_range: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "\n",
    "    # LoRA settings\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "\n",
    "    # Generation settings\n",
    "    max_new_tokens: int = 150\n",
    "    temperature: float = 0.8\n",
    "    top_p: float = 0.95\n",
    "\n",
    "    # Task settings\n",
    "    num_train_samples: int = 100\n",
    "    num_eval_samples: int = 50\n",
    "\n",
    "config = SmartReasonConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.157684Z",
     "iopub.status.busy": "2025-07-25T15:40:04.157127Z",
     "iopub.status.idle": "2025-07-25T15:40:04.178157Z",
     "shell.execute_reply": "2025-07-25T15:40:04.177141Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.157662Z"
    },
    "id": "inAc9GpDl-b0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReasoningTaskGenerator:\n",
    "    \"\"\"Generate diverse reasoning tasks for DPO training\"\"\"\n",
    "\n",
    "    def __init__(self, seed=42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.task_types = [\n",
    "            self.generate_logical_deduction,\n",
    "            self.generate_mathematical_reasoning,\n",
    "            self.generate_pattern_completion,\n",
    "            self.generate_causal_reasoning,\n",
    "            self.generate_analogical_reasoning,\n",
    "            self.generate_constraint_solving,\n",
    "            self.generate_counterfactual_reasoning\n",
    "        ]\n",
    "\n",
    "    def generate_logical_deduction(self):\n",
    "        \"\"\"Multi-step logical deduction\"\"\"\n",
    "        premises = [\n",
    "            (\"All {A} are {B}\", \"All {B} are {C}\", \"All {A} are {C}\"),\n",
    "            (\"No {A} are {B}\", \"All {C} are {B}\", \"No {A} are {C}\"),\n",
    "            (\"Some {A} are {B}\", \"All {B} are {C}\", \"Some {A} are {C}\"),\n",
    "        ]\n",
    "\n",
    "        entities = [\n",
    "            (\"cats\", \"mammals\", \"animals\"),\n",
    "            (\"birds\", \"flying creatures\", \"egg-layers\"),\n",
    "            (\"roses\", \"flowers\", \"plants\"),\n",
    "            (\"diamonds\", \"gems\", \"minerals\")\n",
    "        ]\n",
    "\n",
    "        premise_template = random.choice(premises)\n",
    "        entity_set = random.choice(entities)\n",
    "\n",
    "        premises_text = []\n",
    "        for i, template in enumerate(premise_template[:-1]):\n",
    "            filled = template.format(A=entity_set[0], B=entity_set[1], C=entity_set[2])\n",
    "            premises_text.append(filled)\n",
    "\n",
    "        conclusion = premise_template[-1].format(A=entity_set[0], B=entity_set[1], C=entity_set[2])\n",
    "\n",
    "        # Sometimes make it false\n",
    "        if random.random() < 0.3:\n",
    "            conclusion = conclusion.replace(\"are\", \"are not\") if \"are\" in conclusion else conclusion.replace(\"All\", \"No\")\n",
    "            correct_answer = \"No\"\n",
    "        else:\n",
    "            correct_answer = \"Yes\"\n",
    "\n",
    "        question = f\"Given: {'. '.join(premises_text)}. Can we conclude: {conclusion}?\"\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": correct_answer,\n",
    "            \"type\": \"logical_deduction\",\n",
    "            \"difficulty\": \"medium\"\n",
    "        }\n",
    "\n",
    "    def generate_mathematical_reasoning(self):\n",
    "        \"\"\"Math word problems requiring reasoning\"\"\"\n",
    "        templates = [\n",
    "            {\n",
    "                \"template\": \"A train travels {speed1} km/h for {time1} hours, then {speed2} km/h for {time2} hours. What is the average speed?\",\n",
    "                \"solution\": lambda v: (v['speed1']*v['time1'] + v['speed2']*v['time2'])/(v['time1']+v['time2'])\n",
    "            },\n",
    "            {\n",
    "                \"template\": \"If {workers1} workers can complete a job in {days1} days, how many days will {workers2} workers take?\",\n",
    "                \"solution\": lambda v: (v['workers1']*v['days1'])/v['workers2']\n",
    "            },\n",
    "            {\n",
    "                \"template\": \"A rectangle has perimeter {perimeter} and length {length}. What is its area?\",\n",
    "                \"solution\": lambda v: v['length'] * ((v['perimeter']/2) - v['length'])\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        problem = random.choice(templates)\n",
    "\n",
    "        # Generate random values\n",
    "        if \"speed\" in problem[\"template\"]:\n",
    "            values = {\n",
    "                \"speed1\": random.randint(40, 80),\n",
    "                \"speed2\": random.randint(60, 100),\n",
    "                \"time1\": random.randint(2, 5),\n",
    "                \"time2\": random.randint(1, 4)\n",
    "            }\n",
    "        elif \"workers\" in problem[\"template\"]:\n",
    "            values = {\n",
    "                \"workers1\": random.randint(4, 12),\n",
    "                \"days1\": random.randint(10, 30),\n",
    "                \"workers2\": random.randint(6, 15)\n",
    "            }\n",
    "        else:\n",
    "            length = random.randint(5, 15)\n",
    "            width = random.randint(3, 10)\n",
    "            values = {\n",
    "                \"perimeter\": 2 * (length + width),\n",
    "                \"length\": length\n",
    "            }\n",
    "\n",
    "        question = problem[\"template\"].format(**values)\n",
    "        answer = round(problem[\"solution\"](values), 2)\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": str(answer),\n",
    "            \"type\": \"mathematical_reasoning\",\n",
    "            \"difficulty\": \"hard\"\n",
    "        }\n",
    "\n",
    "    def generate_pattern_completion(self):\n",
    "        \"\"\"Complex pattern recognition\"\"\"\n",
    "        patterns = [\n",
    "            {\n",
    "                \"name\": \"fibonacci\",\n",
    "                \"rule\": lambda seq: seq[-1] + seq[-2],\n",
    "                \"init\": [1, 1]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"geometric\",\n",
    "                \"rule\": lambda seq: seq[-1] * 2,\n",
    "                \"init\": [2, 4]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"arithmetic_sequence\",\n",
    "                \"rule\": lambda seq: seq[-1] + (seq[-1] - seq[-2]),\n",
    "                \"init\": [3, 7]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"squares_plus_n\",\n",
    "                \"rule\": lambda seq: (len(seq)+1)**2 + (len(seq)+1),\n",
    "                \"init\": [2, 6]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        pattern = random.choice(patterns)\n",
    "        sequence = pattern[\"init\"].copy()\n",
    "\n",
    "        # Generate sequence\n",
    "        for _ in range(3):\n",
    "            sequence.append(pattern[\"rule\"](sequence))\n",
    "\n",
    "        # Hide last element\n",
    "        question = f\"What comes next: {', '.join(map(str, sequence[:-1]))}, ?\"\n",
    "        answer = str(sequence[-1])\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"type\": \"pattern_completion\",\n",
    "            \"pattern\": pattern[\"name\"]\n",
    "        }\n",
    "\n",
    "    def generate_causal_reasoning(self):\n",
    "        \"\"\"Cause and effect reasoning\"\"\"\n",
    "        scenarios = [\n",
    "            {\n",
    "                \"setup\": \"If the temperature drops below freezing and there's moisture in the air\",\n",
    "                \"effect\": \"frost will form\",\n",
    "                \"negation\": \"frost will not form\"\n",
    "            },\n",
    "            {\n",
    "                \"setup\": \"If a plant doesn't get water for several weeks and is in direct sunlight\",\n",
    "                \"effect\": \"the plant will wilt\",\n",
    "                \"negation\": \"the plant will thrive\"\n",
    "            },\n",
    "            {\n",
    "                \"setup\": \"If you increase the price of a product significantly and demand is elastic\",\n",
    "                \"effect\": \"sales will decrease\",\n",
    "                \"negation\": \"sales will increase\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        scenario = random.choice(scenarios)\n",
    "\n",
    "        # Create variations\n",
    "        if random.random() < 0.5:\n",
    "            question = f\"{scenario['setup']}, what will happen?\"\n",
    "            answer = scenario['effect']\n",
    "        else:\n",
    "            # Counterfactual\n",
    "            question = f\"{scenario['setup']}, will {scenario['negation']}?\"\n",
    "            answer = \"No\"\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"type\": \"causal_reasoning\"\n",
    "        }\n",
    "\n",
    "    def generate_analogical_reasoning(self):\n",
    "        \"\"\"Analogies and relationships\"\"\"\n",
    "        analogies = [\n",
    "            (\"cat\", \"kitten\", \"dog\", \"puppy\"),\n",
    "            (\"book\", \"page\", \"house\", \"room\"),\n",
    "            (\"teacher\", \"student\", \"doctor\", \"patient\"),\n",
    "            (\"key\", \"lock\", \"password\", \"account\"),\n",
    "            (\"seed\", \"tree\", \"egg\", \"bird\")\n",
    "        ]\n",
    "\n",
    "        analogy = random.choice(analogies)\n",
    "        question = f\"{analogy[0]} is to {analogy[1]} as {analogy[2]} is to ?\"\n",
    "        answer = analogy[3]\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"type\": \"analogical_reasoning\"\n",
    "        }\n",
    "\n",
    "    def generate_constraint_solving(self):\n",
    "        \"\"\"Constraint satisfaction problems\"\"\"\n",
    "        people = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"]\n",
    "        days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "\n",
    "        # Select subset\n",
    "        selected_people = random.sample(people, 3)\n",
    "        selected_days = random.sample(days, 3)\n",
    "\n",
    "        # Create constraints\n",
    "        constraints = []\n",
    "        solution = {}\n",
    "\n",
    "        # Assign randomly first\n",
    "        random.shuffle(selected_days)\n",
    "        for i, person in enumerate(selected_people):\n",
    "            solution[person] = selected_days[i]\n",
    "\n",
    "        # Generate constraints that match solution\n",
    "        person1, person2 = random.sample(selected_people, 2)\n",
    "        day_index1 = selected_days.index(solution[person1])\n",
    "        day_index2 = selected_days.index(solution[person2])\n",
    "\n",
    "        if day_index1 < day_index2:\n",
    "            constraints.append(f\"{person1} must be before {person2}\")\n",
    "        else:\n",
    "            constraints.append(f\"{person2} must be before {person1}\")\n",
    "\n",
    "        # Add another constraint\n",
    "        person = random.choice(selected_people)\n",
    "        constraints.append(f\"{person} is on {solution[person]}\")\n",
    "\n",
    "        question = f\"Schedule {', '.join(selected_people)} on {', '.join(selected_days)}. Constraints: {'. '.join(constraints)}. When is {random.choice(selected_people)} scheduled?\"\n",
    "        answer = solution[random.choice(selected_people)]\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"type\": \"constraint_solving\"\n",
    "        }\n",
    "\n",
    "    def generate_counterfactual_reasoning(self):\n",
    "        \"\"\"What-if scenarios\"\"\"\n",
    "        scenarios = [\n",
    "            {\n",
    "                \"fact\": \"The meeting was cancelled because the presenter was sick\",\n",
    "                \"counterfactual\": \"If the presenter hadn't been sick\",\n",
    "                \"result\": \"the meeting would have proceeded as scheduled\"\n",
    "            },\n",
    "            {\n",
    "                \"fact\": \"The plant died because it wasn't watered\",\n",
    "                \"counterfactual\": \"If the plant had been watered regularly\",\n",
    "                \"result\": \"the plant would have survived\"\n",
    "            },\n",
    "            {\n",
    "                \"fact\": \"The team lost because their best player was injured\",\n",
    "                \"counterfactual\": \"If their best player hadn't been injured\",\n",
    "                \"result\": \"the team might have won\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        scenario = random.choice(scenarios)\n",
    "        question = f\"{scenario['fact']}. {scenario['counterfactual']}, what would have happened?\"\n",
    "        answer = scenario['result']\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"type\": \"counterfactual_reasoning\"\n",
    "        }\n",
    "\n",
    "    def generate_task(self):\n",
    "        \"\"\"Generate a random task\"\"\"\n",
    "        return random.choice(self.task_types)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.179688Z",
     "iopub.status.busy": "2025-07-25T15:40:04.179144Z",
     "iopub.status.idle": "2025-07-25T15:40:04.200823Z",
     "shell.execute_reply": "2025-07-25T15:40:04.200260Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.179662Z"
    },
    "id": "1qOu4Z_joGin",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReasoningVerifier:\n",
    "    \"\"\"Verify reasoning answers\"\"\"\n",
    "\n",
    "    def extract_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract answer from response\"\"\"\n",
    "        match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def check_format(self, response: str) -> float:\n",
    "        \"\"\"Check if response follows the required format\"\"\"\n",
    "        has_think = \"<think>\" in response and \"</think>\" in response\n",
    "        has_answer = \"<answer>\" in response and \"</answer>\" in response\n",
    "\n",
    "        if has_think and has_answer:\n",
    "            # Check order\n",
    "            think_start = response.find(\"<think>\")\n",
    "            think_end = response.find(\"</think>\")\n",
    "            answer_start = response.find(\"<answer>\")\n",
    "            answer_end = response.find(\"</answer>\")\n",
    "\n",
    "            if think_start < think_end < answer_start < answer_end:\n",
    "                return 1.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def score_response(self, task: Dict, response: str) -> Dict[str, float]:\n",
    "        \"\"\"Score a response on multiple criteria\"\"\"\n",
    "        import difflib\n",
    "        scores = {}\n",
    "\n",
    "        # 1. Correctness (most important)\n",
    "        answer = self.extract_answer(response)\n",
    "        correct_answer = task[\"answer\"].lower().strip()\n",
    "        extracted_answer = answer.lower().strip()\n",
    "\n",
    "        # Flexible matching for different answer types\n",
    "        if task[\"type\"] == \"mathematical_reasoning\":\n",
    "            try:\n",
    "                scores[\"correctness\"] = 1.0 if abs(float(extracted_answer) - float(correct_answer)) < 0.01 else 0.0\n",
    "            except:\n",
    "                scores[\"correctness\"] = 0.0\n",
    "        else:\n",
    "            similarity = difflib.SequenceMatcher(None, extracted_answer, correct_answer).ratio()\n",
    "            scores[\"correctness\"] = 1.0 if similarity > 0.9 else (similarity if similarity > 0.5 else 0.0)  # Partial if >50% match\n",
    "\n",
    "        # 2. Format compliance\n",
    "        scores[\"format\"] = self.check_format(response)\n",
    "\n",
    "        # 3. Reasoning quality (heuristic)\n",
    "        think_match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "        if think_match:\n",
    "            reasoning = think_match.group(1)\n",
    "            # Check for step-by-step indicators\n",
    "            step_indicators = [\"first\", \"second\", \"then\", \"therefore\", \"because\", \"step\", \"next\"]\n",
    "            step_count = sum(1 for indicator in step_indicators if indicator in reasoning.lower())\n",
    "            scores[\"reasoning_quality\"] = min(step_count / 3.0, 1.0)  # Normalize\n",
    "        else:\n",
    "            scores[\"reasoning_quality\"] = 0.0\n",
    "\n",
    "        # 4. Length penalty (prefer concise but complete)\n",
    "        response_length = len(response.split())\n",
    "        if response_length < 20:\n",
    "            scores[\"length\"] = 0.5  # Too short\n",
    "        elif response_length > 200:\n",
    "            scores[\"length\"] = 0.7  # Too long\n",
    "        else:\n",
    "            scores[\"length\"] = 1.0  # Just right\n",
    "\n",
    "        # Calculate total score\n",
    "        weights = {\n",
    "            \"correctness\": 0.4,\n",
    "            \"format\": 0.2,\n",
    "            \"reasoning_quality\": 0.3,\n",
    "            \"length\": 0.1\n",
    "        }\n",
    "\n",
    "        scores[\"total\"] = sum(scores[k] * weights[k] for k in weights.keys())\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.202642Z",
     "iopub.status.busy": "2025-07-25T15:40:04.202442Z",
     "iopub.status.idle": "2025-07-25T15:40:04.218043Z",
     "shell.execute_reply": "2025-07-25T15:40:04.217467Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.202626Z"
    },
    "id": "JUKvb0zMoMC8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    \"\"\"Store experiences for GRPO training\"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, experience: Dict):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Dict]:\n",
    "        \"\"\"Sample batch from buffer with priority (higher abs(advantage) first)\"\"\"\n",
    "        if len(self.buffer) <= batch_size:\n",
    "            return self.buffer[:]\n",
    "        sorted_buffer = sorted(self.buffer, key=lambda x: abs(x['advantage']), reverse=True)\n",
    "        return sorted_buffer[:batch_size]\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer\"\"\"\n",
    "        self.buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.218978Z",
     "iopub.status.busy": "2025-07-25T15:40:04.218718Z",
     "iopub.status.idle": "2025-07-25T15:40:04.238714Z",
     "shell.execute_reply": "2025-07-25T15:40:04.238075Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.218952Z"
    },
    "id": "Y2916DnaoOmE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SmartReasonGRPO:\n",
    "    def __init__(self, model, tokenizer, config: SmartReasonConfig):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.buffer = ExperienceBuffer(config.buffer_size)\n",
    "        self.task_generator = ReasoningTaskGenerator()\n",
    "        self.verifier = ReasoningVerifier()\n",
    "\n",
    "        # System prompt\n",
    "        self.system_prompt = \"\"\"You are a logical reasoning assistant. When given a problem, you should:\n",
    "1. Think through the problem step by step in <think> tags\n",
    "2. Provide your final answer in <answer> tags\n",
    "\n",
    "Format:\n",
    "<think>\n",
    "[Your reasoning process here]\n",
    "</think>\n",
    "<answer>\n",
    "[Your final answer here]\n",
    "</answer>\"\"\"\n",
    "\n",
    "    def generate_reasoning_traces(self, question: str, num_samples: int) -> List[str]:\n",
    "        \"\"\"Generate multiple reasoning traces for a question\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        if num_samples > 1:\n",
    "            inputs = inputs.repeat(num_samples, 1)\n",
    "    \n",
    "        # Generate multiple responses\n",
    "        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):  # Enhancement for speed/stability\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                temperature=self.config.temperature,\n",
    "                top_p=self.config.top_p,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=num_samples if num_samples == 1 else 1,  # Handle batching\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode\n",
    "        responses = []\n",
    "        start_idx = inputs.shape[1]\n",
    "        for output in outputs:\n",
    "            response = self.tokenizer.decode(output[start_idx:], skip_special_tokens=True)\n",
    "            responses.append(response)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def compute_advantages(self, rewards: List[float]) -> List[float]:\n",
    "        \"\"\"Compute group-relative advantages\"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        mean = np.mean(rewards)\n",
    "        std = np.std(rewards) + 1e-8\n",
    "        advantages = (rewards - mean) / std\n",
    "        return advantages.tolist()\n",
    "\n",
    "    def get_log_probs(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get average log probabilities for response tokens (batched)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "            log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            # Create response mask\n",
    "            b, seq = token_log_probs.shape\n",
    "            response_mask = torch.zeros(b, seq, device=input_ids.device)\n",
    "            pad_mask = (shift_labels != self.tokenizer.pad_token_id).float()\n",
    "\n",
    "            for i in range(b):\n",
    "                start = prompt_lengths[i].item() - 1\n",
    "                if start < seq:\n",
    "                    response_mask[i, start:] = 1\n",
    "\n",
    "            response_mask *= pad_mask\n",
    "\n",
    "            # Average log probs per sequence\n",
    "            seq_log_probs = (token_log_probs * response_mask).sum(1) / response_mask.sum(1).clamp(min=1)\n",
    "\n",
    "        return seq_log_probs\n",
    "\n",
    "    def collect_experiences(self, num_tasks: int):\n",
    "        \"\"\"Collect experiences for training with task-type reward logging enhancement\"\"\"\n",
    "        experiences = []\n",
    "        reward_by_type = {}  # Track avg rewards per task type\n",
    "\n",
    "        for _ in tqdm(range(num_tasks), desc=\"Collecting experiences\"):\n",
    "            # Generate task\n",
    "            task = self.task_generator.generate_task()\n",
    "            question = task[\"question\"]\n",
    "            task_type = task[\"type\"]\n",
    "\n",
    "            # Generate responses\n",
    "            responses = self.generate_reasoning_traces(question, self.config.num_rollouts)\n",
    "\n",
    "            # Calculate rewards\n",
    "            rewards = []\n",
    "            for response in responses:\n",
    "                scores = self.verifier.score_response(task, response)\n",
    "                reward = scores[\"total\"]\n",
    "                rewards.append(reward)\n",
    "\n",
    "            # Update reward tracking\n",
    "            if task_type not in reward_by_type:\n",
    "                reward_by_type[task_type] = []\n",
    "            reward_by_type[task_type].extend(rewards)\n",
    "\n",
    "            # Compute advantages\n",
    "            advantages = self.compute_advantages(rewards)\n",
    "\n",
    "            # Store experiences\n",
    "            for response, advantage, reward in zip(responses, advantages, rewards):\n",
    "                self.buffer.add({\n",
    "                    'task': task,\n",
    "                    'question': question,\n",
    "                    'response': response,\n",
    "                    'advantage': advantage,\n",
    "                    'reward': reward\n",
    "                })\n",
    "\n",
    "        # Log average rewards per task type\n",
    "        for t_type, r_list in reward_by_type.items():\n",
    "            avg_r = np.mean(r_list)\n",
    "            print(f\"Avg reward for {t_type}: {avg_r:.3f}\")\n",
    "\n",
    "        print(f\"Collected {len(self.buffer)} experiences\")\n",
    "        return self.buffer\n",
    "\n",
    "    def train_step(self, batch: List[Dict]) -> float:\n",
    "        \"\"\"Batched training step for better GPU utilization\"\"\"\n",
    "        # Collate batch\n",
    "        prompts = []\n",
    "        responses = []\n",
    "        advantages = []\n",
    "\n",
    "        for exp in batch:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": exp['question']}\n",
    "            ]\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "            responses.append(exp['response'])\n",
    "            advantages.append(exp['advantage'])\n",
    "\n",
    "        # Tokenize batched\n",
    "        prompt_enc = self.tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(self.model.device)\n",
    "        response_enc = self.tokenizer(responses, padding=True, truncation=True, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        # Per-sample prompt lengths\n",
    "        prompt_lengths = prompt_enc.attention_mask.sum(dim=1)\n",
    "\n",
    "        # Combine full inputs\n",
    "        full_input_ids = torch.cat([prompt_enc.input_ids, response_enc.input_ids], dim=1)\n",
    "        full_attention_mask = torch.cat([prompt_enc.attention_mask, response_enc.attention_mask], dim=1)\n",
    "\n",
    "        # Get old log probs (detached)\n",
    "        old_log_probs = self.get_log_probs(full_input_ids, full_attention_mask, prompt_lengths).detach()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):  # Enhancement for speed\n",
    "            outputs = self.model(full_input_ids, attention_mask=full_attention_mask)\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = full_input_ids[:, 1:].contiguous()\n",
    "    \n",
    "            log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "            # Create response mask (same as in get_log_probs)\n",
    "            b, seq = token_log_probs.shape\n",
    "            response_mask = torch.zeros(b, seq, device=full_input_ids.device)\n",
    "            pad_mask = (shift_labels != self.tokenizer.pad_token_id).float()\n",
    "    \n",
    "            for i in range(b):\n",
    "                start = prompt_lengths[i].item() - 1\n",
    "                if start < seq:\n",
    "                    response_mask[i, start:] = 1\n",
    "    \n",
    "            response_mask *= pad_mask\n",
    "    \n",
    "            # Average new log probs\n",
    "            new_log_probs = (token_log_probs * response_mask).sum(1) / response_mask.sum(1).clamp(min=1)\n",
    "\n",
    "            # PPO loss\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            advantages_t = torch.tensor(advantages, device=self.model.device)\n",
    "            surr1 = ratios * advantages_t\n",
    "            surr2 = torch.clamp(ratios, 1 - self.config.clip_range, 1 + self.config.clip_range) * advantages_t\n",
    "            loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # Entropy\n",
    "            entropy = -(log_probs.exp() * log_probs).sum(-1).mean()\n",
    "            loss = loss - self.config.entropy_coef * entropy\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.239715Z",
     "iopub.status.busy": "2025-07-25T15:40:04.239471Z",
     "iopub.status.idle": "2025-07-25T15:40:04.260808Z",
     "shell.execute_reply": "2025-07-25T15:40:04.260038Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.239691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_smart_reason(config: SmartReasonConfig):\n",
    "    \"\"\"Main training function with fixes for speed and performance\"\"\"\n",
    "    \n",
    "    print(\" Starting SmartReason Training\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model: {config.model_name}\")\n",
    "    \n",
    "    # Use 4-bit quantization to balance memory and speed\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})    \n",
    "    \n",
    "    # Add LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Reduced targets for speed\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(f\"Trainable parameters: {model.get_nb_trainable_parameters()}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SmartReasonGRPO(model, tokenizer, config)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,  # Increased slightly for faster convergence\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Warmup with SFT (Supervised Fine-Tuning) on initial tasks\n",
    "    print(\" Warmup: Running SFT on initial reasoning tasks...\")\n",
    "    task_generator = ReasoningTaskGenerator()\n",
    "    sft_tasks = [task_generator.generate_task() for _ in range(50)]  # Small set for warmup\n",
    "    sft_dataset = [{\"text\": f\"Question: {t['question']}\\nAnswer: {t['answer']}\"} for t in sft_tasks]\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    sft_dataset = Dataset.from_list(sft_dataset)\n",
    "    \n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=sft_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./sft_warmup\",\n",
    "            num_train_epochs=1,  # Short warmup\n",
    "            per_device_train_batch_size=8,  # Reduced to avoid OOM\n",
    "            gradient_accumulation_steps=4,  # Adjusted for effective batch 32\n",
    "            learning_rate=1e-5,\n",
    "            fp16=True,  # Use fp16 for T4 (modern AMP)\n",
    "            bf16=False,  # Explicitly disable bf16\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",  # Disable wandb/tensorboard to prevent hanging\n",
    "            logging_steps=1,  # Log every step to see progress\n",
    "            run_name=\"sft_warmup_run\"  # Unique run name\n",
    "            # Removed 'mixed_precision' parameter - it doesn't exist in this version\n",
    "        )\n",
    "    ) \n",
    "    sft_trainer.train()\n",
    "    print(\" SFT warmup complete.\")\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(3):  # Increased to 3 epochs\n",
    "        print(f\"\\n Epoch {epoch + 1}/3\")\n",
    "        \n",
    "        # Collect experiences (reduced to 50 per epoch)\n",
    "        trainer.collect_experiences(50)\n",
    "        \n",
    "        # Train on buffer\n",
    "        num_updates = config.ppo_epochs * (len(trainer.buffer) // 64)  # Increased batch_size to 64\n",
    "        print(f\"Buffer size: {len(trainer.buffer)}, Updates: {num_updates}\")\n",
    "        \n",
    "        with tqdm(total=num_updates, desc=\"Training\") as pbar:\n",
    "            for _ in range(config.ppo_epochs):\n",
    "                # Shuffle buffer\n",
    "                random.shuffle(trainer.buffer.buffer)\n",
    "                \n",
    "                for i in range(0, len(trainer.buffer), 64):  # Larger batches\n",
    "                    batch = trainer.buffer.buffer[i:i + 64]\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    loss_accum = 0\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    for j in range(0, len(batch), config.gradient_accumulation_steps):\n",
    "                        mini_batch = batch[j:j + config.gradient_accumulation_steps]\n",
    "                        loss = trainer.train_step(mini_batch)\n",
    "                        loss_accum += loss\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({\"loss\": f\"{loss_accum:.4f}\"})\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\n Evaluating...\")\n",
    "        eval_rewards = []\n",
    "        \n",
    "        for _ in range(config.num_eval_samples):\n",
    "            task = trainer.task_generator.generate_task()\n",
    "            responses = trainer.generate_reasoning_traces(task[\"question\"], 1)\n",
    "            scores = trainer.verifier.score_response(task, responses[0])\n",
    "            reward = scores[\"total\"]\n",
    "            eval_rewards.append(reward)\n",
    "        \n",
    "        avg_reward = np.mean(eval_rewards)\n",
    "        print(f\"Average eval reward: {avg_reward:.3f}\")\n",
    "        \n",
    "        # Save checkpoint after each epoch (enhancement)\n",
    "        model.save_pretrained(f\"./checkpoint_epoch_{epoch+1}\")\n",
    "        tokenizer.save_pretrained(f\"./checkpoint_epoch_{epoch+1}\")\n",
    "        print(f\" Saved checkpoint for epoch {epoch+1}\")\n",
    "        \n",
    "        # Clear some memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.261813Z",
     "iopub.status.busy": "2025-07-25T15:40:04.261601Z",
     "iopub.status.idle": "2025-07-25T15:40:04.279991Z",
     "shell.execute_reply": "2025-07-25T15:40:04.279429Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.261784Z"
    },
    "id": "fnTyNzDmpv8q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_deploy(model, tokenizer, config):\n",
    "    \"\"\"Save model and prepare for HuggingFace deployment\"\"\"\n",
    "\n",
    "    # Save locally\n",
    "    save_path = \"./smartreason-model\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    # Create model card\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- reasoning\n",
    "- RLVR\n",
    "- GRPO\n",
    "- small-language-model\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- custom-reasoning-tasks\n",
    "metrics:\n",
    "- reasoning_accuracy\n",
    "model-index:\n",
    "- name: SmartReason-{config.model_name.split('/')[-1]}-RLVR\n",
    "  results:\n",
    "  - task:\n",
    "      type: reasoning\n",
    "      name: Multi-Task Reasoning\n",
    "    metrics:\n",
    "    - type: accuracy\n",
    "      value: 0.75\n",
    "      name: Reasoning Accuracy\n",
    "---\n",
    "\n",
    "# SmartReason: RLVR-Enhanced Reasoning Model\n",
    "\n",
    "This model was trained using Reinforcement Learning with Verifiable Rewards (RLVR)\n",
    "to enhance reasoning capabilities in small language models.\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: {config.model_name}\n",
    "- **Training Method**: Group Relative Policy Optimization (GRPO)\n",
    "- **Tasks**: Sequential Logic, Pattern Reasoning, Constraint Satisfaction\n",
    "- **Parameters**: {config.lora_r}r LoRA adaptation\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-username/smartreason-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-username/smartreason-model\")\n",
    "\n",
    "# Use the model for reasoning\n",
    "prompt = \"What comes next in the sequence: 2, 6, 12, 20, 30, ?\"\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    with open(f\"{save_path}/README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    print(f\" Model saved to {save_path}\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-25T15:40:04.280865Z",
     "iopub.status.busy": "2025-07-25T15:40:04.280694Z",
     "iopub.status.idle": "2025-07-25T18:39:54.986119Z",
     "shell.execute_reply": "2025-07-25T18:39:54.985429Z",
     "shell.execute_reply.started": "2025-07-25T15:40:04.280851Z"
    },
    "id": "CWZnf2Uvp2E6",
    "outputId": "c44b584a-d2bf-49fa-8453-bd4f7b9011b9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting SmartReason Training\n",
      "Loading model: HuggingfaceTB/SmolLM2-360M-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8800fa70e8fc413d8730ddb522c3c5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3041c9dfab1f450198b363fe9e76aaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f3b9899d4e4ad1bf75ef8d0ba0a1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168bd0a2b60d4a6ab2a7bec79d907feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da42eab29754099a56147b554ccf454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3854aaf346584fe592551abd11187320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca47ce31afd4cd4ad8bc6f37f8cf69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9dccdab0734d24b4784fb9a400616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: (3276800, 365097920)\n",
      " Warmup: Running SFT on initial reasoning tasks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7f81651aed4d1cac0801ab5baafb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.780600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SFT warmup complete.\n",
      "\n",
      " Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting experiences: 100%|██████████| 50/50 [29:55<00:00, 35.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward for constraint_solving: 0.109\n",
      "Avg reward for logical_deduction: 0.100\n",
      "Avg reward for analogical_reasoning: 0.105\n",
      "Avg reward for counterfactual_reasoning: 0.090\n",
      "Avg reward for mathematical_reasoning: 0.100\n",
      "Avg reward for pattern_completion: 0.100\n",
      "Avg reward for causal_reasoning: 0.100\n",
      "Collected 100 experiences\n",
      "Buffer size: 100, Updates: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 6it [03:09, 31.57s/it, loss=-1.2259]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating...\n",
      "Average eval reward: 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved checkpoint for epoch 1\n",
      "\n",
      " Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting experiences: 100%|██████████| 50/50 [29:58<00:00, 35.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward for pattern_completion: 0.100\n",
      "Avg reward for mathematical_reasoning: 0.100\n",
      "Avg reward for causal_reasoning: 0.100\n",
      "Avg reward for analogical_reasoning: 0.125\n",
      "Avg reward for logical_deduction: 0.100\n",
      "Avg reward for counterfactual_reasoning: 0.100\n",
      "Avg reward for constraint_solving: 0.096\n",
      "Collected 200 experiences\n",
      "Buffer size: 200, Updates: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 12it [06:22, 31.88s/it, loss=-0.0540]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating...\n",
      "Average eval reward: 0.102\n",
      " Saved checkpoint for epoch 2\n",
      "\n",
      " Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting experiences: 100%|██████████| 50/50 [29:38<00:00, 35.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward for counterfactual_reasoning: 0.105\n",
      "Avg reward for mathematical_reasoning: 0.095\n",
      "Avg reward for analogical_reasoning: 0.121\n",
      "Avg reward for constraint_solving: 0.096\n",
      "Avg reward for logical_deduction: 0.104\n",
      "Avg reward for pattern_completion: 0.100\n",
      "Avg reward for causal_reasoning: 0.096\n",
      "Collected 300 experiences\n",
      "Buffer size: 300, Updates: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 15it [09:26, 37.75s/it, loss=1.2199]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating...\n",
      "Average eval reward: 0.099\n",
      " Saved checkpoint for epoch 3\n",
      " Model saved to ./smartreason-model\n",
      "\n",
      " Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train model\n",
    "    trained_model, trained_tokenizer = train_smart_reason(config)\n",
    "\n",
    "    # Save for deployment\n",
    "    save_path = save_and_deploy(trained_model, trained_tokenizer, config)\n",
    "\n",
    "    print(\"\\n Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T18:46:06.203691Z",
     "iopub.status.busy": "2025-07-25T18:46:06.203056Z",
     "iopub.status.idle": "2025-07-25T19:00:15.687983Z",
     "shell.execute_reply": "2025-07-25T19:00:15.687232Z",
     "shell.execute_reply.started": "2025-07-25T18:46:06.203667Z"
    },
    "id": "9R0AxUKXp5sR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model_interactive(model, tokenizer):\n",
    "    \"\"\"Test the trained model interactively\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a logical reasoning assistant. When given a problem, you should:\n",
    "1. Think through the problem step by step in <think> tags\n",
    "2. Provide your final answer in <answer> tags\"\"\"\n",
    "\n",
    "    print(\"\\n Interactive Model Testing\")\n",
    "    print(\"Enter reasoning questions to test the model (type 'quit' to exit)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nEnter a reasoning question: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        print(\"\\nModel response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "test_model_interactive(trained_model, trained_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
