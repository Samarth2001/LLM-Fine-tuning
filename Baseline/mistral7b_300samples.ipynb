{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Mistral 7B"
      ],
      "metadata": {
        "id": "MIvd7QI9c50U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "TKXvijiDcwPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xujX9iaQm0Vg"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate peft bitsandbytes datasets trl scipy triton\n",
        "!pip install --upgrade transformers==4.41.2 -q\n",
        "!pip install --upgrade peft==0.11.1 -q\n",
        "!pip install --upgrade accelerate==0.30.1 -q\n",
        "!pip install bitsandbytes -q # Removing explicit version to try a different installation method below\n",
        "!pip install --upgrade datasets==2.19.1 -q\n",
        "!pip install --upgrade trl==0.8.6 -q\n",
        "!pip install --upgrade scipy -q\n",
        "!pip install --upgrade triton -q # Let pip handle the triton version based on torch\n",
        "\n",
        "# Attempting to install bitsandbytes from a potentially more compatible source\n",
        "!pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "5k2DDFwGR4jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration and Model Selection"
      ],
      "metadata": {
        "id": "6ojjCvAAuaO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model selection (uncomment one)\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "# MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "# MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_NAME = \"timdettmers/openassistant-guanaco\"\n",
        "DATASET_SPLIT = \"train[:300]\"\n",
        "\n",
        "# Training parameters (conservative for 7B)\n",
        "LORA_R = 8\n",
        "BATCH_SIZE = 1\n",
        "SEQ_LENGTH = 512\n",
        "EPOCHS = 1\n",
        "\n",
        "print(f\"--- Configuration ---\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Samples: {DATASET_SPLIT}\")\n",
        "print(f\"LoRA Rank: {LORA_R}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"-----------------------\")"
      ],
      "metadata": {
        "id": "tcYcJttOuxWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and GPU Check"
      ],
      "metadata": {
        "id": "uGGfUc66dhZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Clear Memory & Check GPU\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"--- GPU Information ---\")\n",
        "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"Current Usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"-----------------------\")\n",
        "else:\n",
        "    print(\"No GPU detected. This will be very slow.\")\n",
        "    raise RuntimeError(\"GPU required for 7B model fine-tuning\")\n",
        "\n",
        "# Memory tracking function\n",
        "def print_gpu_memory(stage=\"\"):\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"\\n--- GPU Memory {stage} ---\")\n",
        "    print(f\"Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"--------------------------\\n\")"
      ],
      "metadata": {
        "id": "GGReiBy7R4gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Quantization"
      ],
      "metadata": {
        "id": "ghnciZK3dkY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"Quantization configured for 4-bit loading.\")"
      ],
      "metadata": {
        "id": "ztAhap_SvBkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "0ArCCZ1Pdncd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLoading {MODEL_NAME}...\")\n",
        "start_load_time = time.time()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "    print(f\"Model loaded successfully in {time.time() - start_load_time:.1f} seconds.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Falling back to Phi-2...\")\n",
        "    MODEL_NAME = \"microsoft/phi-2\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "\n",
        "print_gpu_memory(\"After Model Loading\")"
      ],
      "metadata": {
        "id": "jw0TccofvFHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Model for Training"
      ],
      "metadata": {
        "id": "eidzJVG1dqNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"Model prepared for training.\")"
      ],
      "metadata": {
        "id": "pZcd6LVPwfjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure LoRA"
      ],
      "metadata": {
        "id": "8qsNM3wPduDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine target modules based on model type\n",
        "if \"mistral\" in MODEL_NAME.lower():\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "elif \"llama\" in MODEL_NAME.lower():\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "elif \"phi\" in MODEL_NAME.lower():\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        "else:\n",
        "    # Default modules\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
        "\n",
        "print(f\"Target modules for LoRA: {target_modules}\")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_R * 2,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=target_modules\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
        "print(f\"\\nLoRA Applied Successfully!\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "print_gpu_memory(\"After LoRA Application\")"
      ],
      "metadata": {
        "id": "Or3ob7kQwhTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "9sXYIGvQdxiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLoading dataset: {DATASET_NAME}\")\n",
        "print(f\"Using samples: {DATASET_SPLIT}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
        "\n",
        "# Format function for the dataset\n",
        "def format_dataset(example):\n",
        "    # Handle different dataset formats\n",
        "    if \"instruction\" in example and \"output\" in example:\n",
        "        # Instruction-following format\n",
        "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
        "    elif \"human\" in example and \"assistant\" in example:\n",
        "        # Conversational format\n",
        "        text = f\"### Human: {example['human']}\\n### Assistant: {example['assistant']}\"\n",
        "    elif \"text\" in example:\n",
        "        # Plain text format\n",
        "        text = example[\"text\"]\n",
        "    else:\n",
        "        # Fallback\n",
        "        text = str(example)\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(format_dataset)\n",
        "\n",
        "# Split into train and validation\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Dataset prepared.\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample text preview:\")\n",
        "print(train_dataset[0][\"text\"][:500] + \"...\")"
      ],
      "metadata": {
        "id": "O-WV6xUEwo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Training Arguments"
      ],
      "metadata": {
        "id": "Kr5M5_1Ld0ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gradient accumulation steps to maintain effective batch size\n",
        "effective_batch_size = 8\n",
        "gradient_accumulation_steps = effective_batch_size // BATCH_SIZE\n",
        "\n",
        "print(f\"--- Training Configuration ---\")\n",
        "print(f\"Per device batch size: {BATCH_SIZE}\")\n",
        "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {effective_batch_size}\")\n",
        "print(f\"------------------------------\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_7b\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=[],\n",
        "    group_by_length=True,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining arguments configured.\")"
      ],
      "metadata": {
        "id": "OSeCpkjRw0F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Trainer"
      ],
      "metadata": {
        "id": "jFqZVCY-d3OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    max_seq_length=SEQ_LENGTH,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"Trainer created successfully.\")\n",
        "print_gpu_memory(\"Before Training\")"
      ],
      "metadata": {
        "id": "vWYckIRbw3n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with Monitoring"
      ],
      "metadata": {
        "id": "kmyv4us8d6I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear cache before training\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Calculate training statistics\n",
        "    training_time = (time.time() - start_time) / 60\n",
        "    final_loss = train_result.training_loss\n",
        "    samples_per_second = len(train_dataset) / (time.time() - start_time)\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "    print(f\"\\n--- Training Statistics ---\")\n",
        "    print(f\"Total time: {training_time:.1f} minutes\")\n",
        "    print(f\"Final loss: {final_loss:.4f}\")\n",
        "    print(f\"Samples/second: {samples_per_second:.2f}\")\n",
        "\n",
        "    # Check if we should scale up\n",
        "    max_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
        "    print(f\"Max GPU memory: {max_memory:.2f} GB\")\n",
        "    print(f\"---------------------------\")\n",
        "\n",
        "    if max_memory < 13 and final_loss < 2.0:\n",
        "        print(f\"\\nSuccess: Model trained well with headroom.\")\n",
        "        print(f\"Next step: Increase dataset to 500 samples\")\n",
        "    elif max_memory > 14:\n",
        "        print(f\"\\nWarning: Memory usage high!\")\n",
        "        print(f\"Next step: Keep current settings or reduce LoRA rank\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: Training failed with error: {e}\")\n",
        "    print(f\"Suggestions:\")\n",
        "    print(f\"- Reduce LoRA rank to 4\")\n",
        "    print(f\"- Reduce Sequence length to 256\")\n",
        "    print(f\"- Reduce Dataset size to 200\")\n",
        "\n",
        "print_gpu_memory(\"After Training\")"
      ],
      "metadata": {
        "id": "ARlA4xsxxL5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "kiacSS84d98i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "if 'train_result' in locals():\n",
        "    print(\"\\nSaving model...\")\n",
        "\n",
        "    save_path = f\"./{MODEL_NAME.split('/')[-1]}-finetuned\"\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    # Calculate adapter size\n",
        "    import os\n",
        "    adapter_size = sum(os.path.getsize(os.path.join(save_path, f))\n",
        "                      for f in os.listdir(save_path)\n",
        "                      if f.endswith('.bin') or f.endswith('.safetensors')) / 1024**2\n",
        "\n",
        "    print(f\"Adapter size: {adapter_size:.1f} MB\")"
      ],
      "metadata": {
        "id": "7aCsWVQ0xXpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Fine-tuned Model"
      ],
      "metadata": {
        "id": "B_f3w5zEeQpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting the fine-tuned model...\")\n",
        "model.config.use_cache = True\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"### Instruction:\\nExplain quantum computing in simple terms.\\n\\n### Response:\\n\",\n",
        "    \"### Instruction:\\nWrite a Python function to calculate fibonacci numbers.\\n\\n### Response:\\n\",\n",
        "    \"### Instruction:\\nWhat are the benefits of exercise?\\n\\n### Response:\\n\"\n",
        "]\n",
        "\n",
        "# Select prompt based on model performance\n",
        "if 'final_loss' in locals() and final_loss < 1.5:\n",
        "    prompt = test_prompts[1]\n",
        "else:\n",
        "    prompt = test_prompts[0]\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "generated_text = response[len(prompt):]\n",
        "\n",
        "print(\"Generated Response:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "777joGoexchh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Summary and Next Steps"
      ],
      "metadata": {
        "id": "ZLT0mNB1eNtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create summary\n",
        "summary = {\n",
        "    \"Model\": MODEL_NAME,\n",
        "    \"Parameters\": \"7B\" if \"7b\" in MODEL_NAME.lower() else \"2.7B\",\n",
        "    \"Dataset Size\": DATASET_SPLIT,\n",
        "    \"LoRA Rank\": LORA_R,\n",
        "    \"Training Time\": f\"{training_time:.1f} min\" if 'training_time' in locals() else \"N/A\",\n",
        "    \"Final Loss\": f\"{final_loss:.4f}\" if 'final_loss' in locals() else \"N/A\",\n",
        "    \"Max GPU Memory\": f\"{max_memory:.2f} GB\" if 'max_memory' in locals() else \"N/A\",\n",
        "    \"Status\": \"Success\" if 'train_result' in locals() else \"Failed\"\n",
        "}\n",
        "\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key:.<20} {value}\")\n",
        "\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "if 'train_result' in locals() and max_memory < 13:\n",
        "    print(\"1. Increase dataset size to 500 samples\")\n",
        "    print(\"2. Try LoRA rank 16 for better quality\")\n",
        "    print(\"3. Experiment with different datasets\")\n",
        "    print(\"\\nUpdate these values in Cell 2:\")\n",
        "    print('   DATASET_SPLIT = \"train[:500]\"')\n",
        "    print('   LORA_R = 16')\n",
        "elif 'max_memory' in locals() and max_memory > 13:\n",
        "    print(\"1. Memory is tight, optimize further:\")\n",
        "    print(\"2. Set LORA_R = 4\")\n",
        "    print(\"3. Set SEQ_LENGTH = 256\")\n",
        "    print(\"4. Keep dataset at 300 samples\")\n",
        "else:\n",
        "    print(\"1. Debug the error\")\n",
        "    print(\"2. Try with Phi-2 model first\")\n",
        "    print(\"3. Reduce all parameters\")\n",
        "\n",
        "print(\"\\nPro tip: Save this notebook with results before next experiment!\")"
      ],
      "metadata": {
        "id": "FLpfZ0vExhn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick configuration switcher for multiple experiments\n",
        "CONFIGS = {\n",
        "    \"conservative_7b\": {\n",
        "        \"MODEL_NAME\": \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"DATASET_SPLIT\": \"train[:300]\",\n",
        "        \"LORA_R\": 8,\n",
        "        \"BATCH_SIZE\": 1,\n",
        "        \"SEQ_LENGTH\": 512\n",
        "    },\n",
        "    \"scaled_7b\": {\n",
        "        \"MODEL_NAME\": \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"DATASET_SPLIT\": \"train[:500]\",\n",
        "        \"LORA_R\": 8,\n",
        "        \"BATCH_SIZE\": 1,\n",
        "        \"SEQ_LENGTH\": 512\n",
        "    },\n",
        "    \"memory_optimized\": {\n",
        "        \"MODEL_NAME\": \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"DATASET_SPLIT\": \"train[:200]\",\n",
        "        \"LORA_R\": 4,\n",
        "        \"BATCH_SIZE\": 1,\n",
        "        \"SEQ_LENGTH\": 256\n",
        "    },\n",
        "    \"fallback_phi2\": {\n",
        "        \"MODEL_NAME\": \"microsoft/phi-2\",\n",
        "        \"DATASET_SPLIT\": \"train[:2000]\",\n",
        "        \"LORA_R\": 16,\n",
        "        \"BATCH_SIZE\": 2,\n",
        "        \"SEQ_LENGTH\": 512\n",
        "    }\n",
        "}\n",
        "\n",
        "# To use: Copy the config you want to Cell 2\n",
        "print(\"Available configurations:\")\n",
        "for name, config in CONFIGS.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key} = {value}\")"
      ],
      "metadata": {
        "id": "mNIh3GDS4Ltl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}