{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xujX9iaQm0Vg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate peft bitsandbytes datasets trl scipy triton\n",
        "!pip install --upgrade transformers==4.41.2 -q\n",
        "!pip install --upgrade peft==0.11.1 -q\n",
        "!pip install --upgrade accelerate==0.30.1 -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install --upgrade datasets==2.19.1 -q\n",
        "!pip install --upgrade trl==0.8.6 -q\n",
        "!pip install --upgrade scipy -q\n",
        "!pip install --upgrade triton -q\n",
        "\n",
        "!pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "5k2DDFwGR4jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected. This will be very slow.\")\n"
      ],
      "metadata": {
        "id": "GGReiBy7R4gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # bf16 is optimal for Ampere GPUs like T4\n",
        ")\n",
        "\n",
        "print(\"\\n Loading model and tokenizer...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Set a padding token if one is not already defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically place the model on the available device (GPU)\n",
        "    trust_remote_code=True,\n",
        "    use_cache=False, # Disable caching for training\n",
        ")\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "jX1lo5IPR4eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA (Low-Rank Adaptation) is a technique to efficiently fine-tune large models.\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                   # Rank of the update matrices. Lower means fewer parameters.\n",
        "    lora_alpha=32,          # A scaling factor for the LoRA weights.\n",
        "    lora_dropout=0.05,      # Dropout probability for LoRA layers.\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print the percentage of trainable parameters\n",
        "trainable_params, total_params = model.get_nb_trainable_parameters()\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cYKAKIMuR4cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nðŸ”„ Loading and preparing dataset...\")\n",
        "# dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
        "# print(f\"âœ… Dataset loaded with {len(dataset)} samples.\")\n",
        "\n",
        "\n",
        "# --- 5. Load and Prepare Dataset ---\n",
        "print(\"\\nðŸ”„ Loading and preparing dataset...\")\n",
        "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
        "# **NEW** Split dataset into training and validation sets\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "print(f\"âœ… Training samples: {len(train_dataset)}, Evaluation samples: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "2e7_3phhR4aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,      # Can increase slightly if memory allows\n",
        "    gradient_accumulation_steps=4,      # Effective batch size of 8\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=25,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True, # Loads the best model found during training\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=[],\n",
        ")"
      ],
      "metadata": {
        "id": "dyRa6Qc8R4YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Starting training...\")\n",
        "# Clear cache one last time before training\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Train the model! This should take about 5-10 minutes on a T4.\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nðŸŽ‰ Training completed!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pl5Yae-eR4VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ§ª Testing the fine-tuned model...\")\n",
        "model.config.use_cache = True\n",
        "\n",
        "prompt = \"### Instruction:\\nWrite a short story about a robot who discovers music.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=250, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode and print the output\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- PROMPT ---\")\n",
        "print(prompt)\n",
        "print(\"\\n--- MODEL RESPONSE ---\")\n",
        "print(response_text[len(prompt):]) # Print only the newly generated part\n"
      ],
      "metadata": {
        "id": "2FjUOt6hR4TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9Ixhll6R4RW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}